{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29a5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lg\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from typing import List, TypeVar, Dict\n",
    "import abc\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, precision_score, average_precision_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from kydavra import FisherSelector\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98a9a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78db6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_СORRELATION_MATRIX_PATH_ = 'corr_matrix.pcl'\n",
    "_СHURN_PATH_ = 'churn_model.pcl'\n",
    "_CHURN_PATH_REDUCED_ = 'churn_model_reduced.pcl' \n",
    "_MAX_FEATURES_ = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dabd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077\n",
    "# https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "class Metrics:\n",
    "    \n",
    "    def roc_auc(y_true, predicted):\n",
    "        return roc_auc_score(y_true, predicted)\n",
    "    \n",
    "    def auc(y_true, predicted):\n",
    "        return average_precision_score(y_true, predicted)\n",
    "    \n",
    "    def classification_report(y_true, predicted):\n",
    "        return classification_report(y_true, predicted)\n",
    "    \n",
    "    def confusion_matrix(y_true, predicted):\n",
    "        return confusion_matrix(y_true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1480700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_float_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=float).columns.tolist()\n",
    "\n",
    "def get_int_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=int).columns.tolist()\n",
    "\n",
    "def get_number_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(np.number).columns.tolist()\n",
    "\n",
    "def get_obj_cols(df:DataFrame) -> List[str]:\n",
    "    return list(df.select_dtypes(include=object).columns)\n",
    "\n",
    "def print_empty_values(df:DataFrame):\n",
    "    col_names_with_na = list(df.isna().sum()[lambda x: x > 0].index)\n",
    "    col_names_with_empty = list(df.isnull().sum()[lambda x: x > 0].index)\n",
    "    result = set(col_names_with_na) | set(col_names_with_empty)    \n",
    "    print('Columns with NA or empty: {0}'.format(result))\n",
    "    \n",
    "def get_empty_cols(df:DataFrame):\n",
    "    return list(df.isnull().sum()[lambda x: x > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1207571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True) -> pd.DataFrame:\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0ffcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(df:DataFrame):\n",
    "    test, train = df[df['ind'].eq('test')], df[df['ind'].eq('train')]\n",
    "    test = test.drop(['ind'], axis=1)\n",
    "    train = train.drop(['ind'], axis=1)\n",
    "    return test, train\n",
    "    \n",
    "def combine_test_train(test:DataFrame, train:DataFrame):\n",
    "    combine = pd.concat([test.assign(ind='test'), train.assign(ind='train')])\n",
    "    target = train['target']\n",
    "    test_ids = test['Id']\n",
    "    return combine, target, test_ids\n",
    "\n",
    "def combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi):\n",
    "    df_combine = pd.concat([train_df.assign(ind='train'), test_df.assign(ind='test')])\n",
    "    df_combine_num = pd.concat([train_num.assign(ind='train'), test_num.assign(ind='test')])\n",
    "    df_combine_dpi = pd.concat([train_dpi.assign(ind='train'), test_dpi.assign(ind='test')])\n",
    "    return df_combine, df_combine_num, df_combine_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f25481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_churn_data():\n",
    "\n",
    "    if os.path.exists(_CHURN_PATH_REDUCED_) == False:\n",
    "        if os.path.exists(_СHURN_PATH_) == True: \n",
    "    \n",
    "            with open(_СHURN_PATH_, 'rb') as file:\n",
    "                deserialized_object = pickle.load(file)\n",
    "\n",
    "            deserialized_object = list(deserialized_object)\n",
    "        \n",
    "            # reduce size\n",
    "            train_df= reduce_mem_usage(deserialized_object[1][1])\n",
    "            train_num_reduced = reduce_mem_usage(deserialized_object[1][2])\n",
    "            train_dpi_reduced = reduce_mem_usage(deserialized_object[1][3])\n",
    "\n",
    "            test_df = reduce_mem_usage(deserialized_object[2][1])\n",
    "            test_num_reduced = reduce_mem_usage(deserialized_object[2][2])\n",
    "            test_dpi_reduced = reduce_mem_usage(deserialized_object[2][3])\n",
    "            \n",
    "            # dump data back\n",
    "            deserialized_object = tuple([[train_df, train_num_reduced, train_dpi_reduced], [test_df, test_num_reduced, test_dpi_reduced]])\n",
    "            pickle.dump(deserialized_object, open(_CHURN_PATH_REDUCED_, 'wb'))\n",
    "    else:\n",
    "        print(f'{_CHURN_PATH_REDUCED_} already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6e18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_data():\n",
    "\n",
    "    with open(_СHURN_PATH_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[1][1]\n",
    "    train_num = deserialized_object[1][2]\n",
    "    train_dpi = deserialized_object[1][3]\n",
    "\n",
    "    test_df = deserialized_object[2][1]\n",
    "    test_num = deserialized_object[2][2]\n",
    "    test_dpi = deserialized_object[2][3]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d952ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_reduced():\n",
    "\n",
    "    with open(_CHURN_PATH_REDUCED_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[0][0]\n",
    "    train_num = deserialized_object[0][1]\n",
    "    train_dpi = deserialized_object[0][2]\n",
    "\n",
    "    test_df = deserialized_object[1][0]\n",
    "    test_num = deserialized_object[1][1]\n",
    "    test_dpi = deserialized_object[1][2]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f00265aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_short_number(number:str) -> bool:\n",
    "    if (number.isdigit() and len(number) <= 4):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_life(number:str) -> bool:\n",
    "    if (len(number) == 12 and (number[2:5] in ['063', '093' ])):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_kyivstar(number:str) -> bool:\n",
    "    if (len(number) == 12 and (number[2:5] in ['067', '097', '068', '098'])):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca85d64",
   "metadata": {},
   "source": [
    "### Feature selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7024508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureHelper:\n",
    "    \n",
    "    def get_feature_correlation_df(corr_m, remove_duplicates=True, remove_self_correlations=True):\n",
    "    \n",
    "        corr_matrix_abs = corr_m.abs()\n",
    "        corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
    "        sorted_correlated_features = corr_matrix_abs_us \\\n",
    "            .sort_values(kind=\"quicksort\", ascending=False) \\\n",
    "            .reset_index()\n",
    "\n",
    "        # Remove comparisons of the same feature\n",
    "        if remove_self_correlations:\n",
    "            sorted_correlated_features = sorted_correlated_features[\n",
    "                (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
    "            ]\n",
    "\n",
    "        # Remove duplicates\n",
    "        if remove_duplicates:\n",
    "            sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
    "\n",
    "        # Create meaningful names for the columns\n",
    "        sorted_correlated_features.columns = ['f1', 'f2', 'corr']\n",
    "\n",
    "        return sorted_correlated_features\n",
    "    \n",
    "    def get_correlation_matrix(df:DataFrame, method:str, save_path:str):\n",
    "        if os.path.exists(save_path) == False:\n",
    "            corr_matrix = df.corr(method = method, numeric_only = True)\n",
    "            pickle.dump(corr_matrix, open(save_path, 'wb'))\n",
    "        else:\n",
    "            corr_matrix = pickle.load(open(save_path, 'rb'))\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def remove_aggr_function(str_to_check:str) -> str:\n",
    "        parts = str_to_check.split('_')\n",
    "        \n",
    "        if (len(parts) > 2):\n",
    "            index_to_remove = len(parts) - 2\n",
    "            \n",
    "            # remove aggregation function\n",
    "            if (parts[index_to_remove] in ['min', 'std', 'max', 'mea', 'td']):\n",
    "                parts.remove(parts[index_to_remove])\n",
    "                \n",
    "            result = '_'.join(parts)\n",
    "            return result\n",
    "        else:\n",
    "            return str_to_check    \n",
    "\n",
    "        \n",
    "    def get_heatmap_matrix(corr_matrix:DataFrame):\n",
    "        heatmap_matrix = pd.DataFrame(corr_matrix['target'].abs())\n",
    "        heatmap_matrix = heatmap_matrix.sort_values(by='target', ascending=False)\n",
    "        heatmap_matrix = heatmap_matrix.drop(index=['target'])           \n",
    "        return heatmap_matrix\n",
    "    \n",
    "    # index - column name\n",
    "    # target - value\n",
    "    def plot_heatmap(heatmap_matrix:DataFrame):\n",
    "        plt.figure(figsize=(40, 120))\n",
    "        heatmap = sns.heatmap(heatmap_matrix, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "        heatmap.set_title('Features Correlating with Churn Rate', fontdict={'fontsize':18}, pad=16);\n",
    "        return heatmap_matrix\n",
    "    \n",
    "    def get_important_features(heatmap_matrix:DataFrame, use_groupping = False, num_of_features:int = -1):\n",
    "        df_features = heatmap_matrix.reset_index()\n",
    "        df_features = df_features.rename(columns = {'index':'feature'})\n",
    "        \n",
    "        # apply aggregation function for further groupping\n",
    "        df_features['feature_group'] = df_features['feature'].apply(FeatureHelper.remove_aggr_function)\n",
    "        df_features = df_features[['feature', 'feature_group', 'target']]\n",
    "        sorted_features = df_features.sort_values(by=['feature_group', 'target'], ascending = [False, False])\n",
    "        \n",
    "        # take first item from the group\n",
    "        if (use_groupping == True):\n",
    "            important_features = sorted_features.groupby('feature_group').first()\n",
    "        else:\n",
    "            important_features = sorted_features\n",
    "\n",
    "        # order by target\n",
    "        important_features = important_features.sort_values(by='target', ascending=False)\n",
    "           \n",
    "        # take N first rows\n",
    "        if (num_of_features != -1):\n",
    "            important_features = important_features.head(num_of_features)\n",
    "        \n",
    "        # optimize for heatmap\n",
    "        important_features = important_features.reset_index()\n",
    "        important_features = important_features[['feature', 'target']]\n",
    "        important_features.index = important_features['feature']\n",
    "        important_features.index.name = None\n",
    "        important_features = important_features[['target']]\n",
    "        important_features = important_features[important_features['target'] > 0]\n",
    "        \n",
    "        return important_features\n",
    "    \n",
    "    def get_important_features_tuples(heatmap_matrix:DataFrame, num_of_features:int = -1):\n",
    "        important_features = FeatureHelper.get_important_features(heatmap_matrix, num_of_features)\n",
    "        \n",
    "        if (num_of_features == -1):\n",
    "            num_of_features = len(important_features)\n",
    "        \n",
    "        important_features_tuples = list(zip(important_features.index, \n",
    "                                             important_features.target, \n",
    "                                             list(range(0, num_of_features))))\n",
    "        \n",
    "        return important_features_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b170f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def plot_heatmap(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    # 1 - feature name, 2 - target, 3 - sorted number\n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe1177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationSelector(FeatureSelector):\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, corr_method:str, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.corr_method = corr_method\n",
    "        self.file_prefix = corr_method\n",
    "        self.num_of_features = num_of_features\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        self.save_path = f'{self.file_prefix}_{_СORRELATION_MATRIX_PATH_}'\n",
    "        self.corr_m = FeatureHelper.get_correlation_matrix(self.data, self.corr_method, self.save_path)\n",
    "        self.heatmap_m = FeatureHelper.get_heatmap_matrix(self.corr_m)\n",
    "        return self.heatmap_m \n",
    "    \n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "    \n",
    "    def get_non_correlated_features(self, barrier_coef:float, do_log:bool) -> List[str]:\n",
    "        \n",
    "        important_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        # f1, f2, corr\n",
    "        features_corr = FeatureHelper.get_feature_correlation_df(self.corr_m)\n",
    "        \n",
    "        already_processed = set()\n",
    "        all_features = [t[0] for t in important_tuples]\n",
    "\n",
    "        for f in all_features:\n",
    "            # get correlated features\n",
    "            correlated = list(features_corr[(features_corr['f1']==f) & (features_corr['corr'] > barrier_coef)]['f2'])\n",
    "\n",
    "            # if highly correlated features exist\n",
    "            if (len(correlated)>0):\n",
    "\n",
    "                for to_remove in correlated:\n",
    "                    if (to_remove not in already_processed):\n",
    "                        if (to_remove in all_features):\n",
    "                            all_features.remove(to_remove)\n",
    "                            if do_log: print(f'Removing: {to_remove} for {f}')            \n",
    "\n",
    "        # remember initial feature\n",
    "        already_processed.add(f)\n",
    "        \n",
    "        # return non-correlated features\n",
    "        return all_features\n",
    "    \n",
    "    def get_important_noncorrelated_features_tuples(self, mutual_correlation:float, feature_importance:float):\n",
    "        \n",
    "        final = []\n",
    "        important = self.get_important_features_tuples()\n",
    "        non_correlated = self.get_non_correlated_features(mutual_correlation, False)\n",
    "        \n",
    "        for i in important:\n",
    "            if (i[0] in non_correlated and i[1] > feature_importance):\n",
    "                final.append(i)\n",
    "\n",
    "        return final\n",
    "                        \n",
    "    def get_important_features_tuples(self) ->  List[tuple[str, float, int]]:\n",
    "        return FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return type(self).__name__ + '_' + self.corr_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01bd308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nitin9809.medium.com/lightgbm-binary-classification-multi-class-classification-regression-using-python-4f22032b36a2\n",
    "# https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n",
    "class LGBMSelector(FeatureSelector):\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.num_of_features = num_of_features\n",
    "        self.train_model()\n",
    "    \n",
    "    def train_model(self):\n",
    "        test, train = split_test_train(self.data)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        train = train.drop(['target'], axis=1)\n",
    "        test = test.drop(['target'], axis=1)\n",
    "\n",
    "        # save regressor\n",
    "        self.regressor = lg.LGBMClassifier()\n",
    "        self.regressor.fit(train, self.y_train)\n",
    "        predicted = self.regressor.predict(test)\n",
    "        \n",
    "        # save predicted data\n",
    "        self.predicted = predicted\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        df_feature_importance = pd.DataFrame(list(zip(list(self.regressor.feature_importances_), list(self.data.columns))))\n",
    "        df_feature_importance = df_feature_importance.set_axis(['target', 'feature'], axis=1)\n",
    "        df_feature_importance = df_feature_importance.sort_values(by=['target'], ascending=False)\n",
    "        df_feature_importance = df_feature_importance.set_index('feature')\n",
    "        df_feature_importance.index.name = None\n",
    "        self.heatmap_m = df_feature_importance\n",
    "        return self.heatmap_m\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        features_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        return features_tuples\n",
    "        \n",
    "    def get_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_test, self.predicted)\n",
    "\n",
    "    def get_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_test, self.predicted)\n",
    "    \n",
    "    def get_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_test, self.predicted)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return type(self).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7ab0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "class EnhancedLGBMSelector(LGBMSelector):\n",
    "    \n",
    "    def train_model(self):        \n",
    "        \n",
    "        test, train = split_test_train(df_combine)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        self.x_train = train.drop(['target'], axis=1)\n",
    "        self.x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "        # Specifying the parameter\n",
    "        d_train = lgb.Dataset(x_train, label=y_train)\n",
    "        d_test = lgb.Dataset(x_test, label=y_test, reference=d_train)\n",
    "\n",
    "        # load model from disk\n",
    "        if os.path.exists(_СHURN_PATH_) == True: \n",
    "            with open('LGBM_model_800_features.pcl', 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "        else:\n",
    "            params={}\n",
    "            params['boosting_type']='gbdt' \n",
    "            params['objective']='binary' \n",
    "            params['metric']='auc' \n",
    "            params['verbosity'] = 0\n",
    "\n",
    "            #train the model \n",
    "            self.regressor=lgb.train(params = params, \n",
    "                          train_set = d_train,\n",
    "                          valid_sets = d_test, \n",
    "                          num_boost_round = 20, \n",
    "                          callbacks= [lgb.early_stopping(stopping_rounds=10)])\n",
    "        \n",
    "        #prediction on the test set\n",
    "        self.predicted = self.regressor.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "effae5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformPipe:\n",
    "    \n",
    "    def __init__(self, funcs, **kwargs):\n",
    "        self.funcs = funcs\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def transform(self, df:DataFrame) -> DataFrame:\n",
    "        for f in self.funcs:\n",
    "            df = f(df, **self.kwargs)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688eb39d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d3baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_num, train_dpi, test_df, test_num, test_dpi = load_churn_reduced()\n",
    "df_combine, df_combine_num, df_combine_dpi = combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f98699",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3216c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43\n",
    "# https://www.kaggle.com/code/gomes555/tps-jun2021-feature-selection-lightgbm-tuner\n",
    "\n",
    "result = {}\n",
    "seleсtors = [ CorrelationSelector(df_combine, 'pearson', -1),\n",
    "              CorrelationSelector(df_combine, 'spearman', -1) ]\n",
    "\n",
    "for selector in seleсtors:\n",
    "    heatmap = selector.get_heatmap()\n",
    "    features = selector.get_important_features()\n",
    "    features_tuples = selector.get_important_features_tuples()\n",
    "    result[selector.__str__()] = selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0c84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal = LGBMSelector(df_combine, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a31276b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced = EnhancedLGBMSelector(df_combine, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e549b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal.get_ROCAUC(), enhanced.get_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d64c8c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:28:47,743] A new study created in memory with name: no-name-88c2c11e-f6e1-4583-839a-02341f78fa75\n",
      "feature_fraction, val_score: -inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.355101 seconds, init for row-wise cost 1.666131 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.950028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.883539:  14%|#4        | 1/7 [00:31<03:08, 31.39s/it][I 2023-07-01 00:29:19,177] Trial 0 finished with value: 0.883539274986546 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.883539274986546.\n",
      "feature_fraction, val_score: 0.883539:  14%|#4        | 1/7 [00:31<03:08, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.883539\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.291937 seconds, init for row-wise cost 1.293693 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.515542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879:  29%|##8       | 2/7 [01:00<02:29, 29.87s/it][I 2023-07-01 00:29:48,004] Trial 1 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 0.8}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879:  29%|##8       | 2/7 [01:00<02:29, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.323685 seconds, init for row-wise cost 1.411335 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.660999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879:  43%|####2     | 3/7 [01:32<02:03, 30.77s/it][I 2023-07-01 00:30:19,854] Trial 2 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879:  43%|####2     | 3/7 [01:32<02:03, 30.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.274004 seconds, init for row-wise cost 1.373030 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.431372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879:  57%|#####7    | 4/7 [02:03<01:33, 31.13s/it][I 2023-07-01 00:30:51,527] Trial 3 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 0.4}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879:  57%|#####7    | 4/7 [02:03<01:33, 31.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.335733 seconds, init for row-wise cost 1.439177 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.638092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879:  71%|#######1  | 5/7 [02:30<00:58, 29.41s/it][I 2023-07-01 00:31:17,846] Trial 4 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879:  71%|#######1  | 5/7 [02:30<00:58, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.274508 seconds, init for row-wise cost 1.244605 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.447546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879:  86%|########5 | 6/7 [02:55<00:28, 28.22s/it][I 2023-07-01 00:31:43,816] Trial 5 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879:  86%|########5 | 6/7 [02:56<00:28, 28.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.299129 seconds, init for row-wise cost 1.360175 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.390937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Sparse Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.886879: 100%|##########| 7/7 [03:26<00:00, 28.85s/it][I 2023-07-01 00:32:13,954] Trial 6 finished with value: 0.8868785101089008 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.8868785101089008.\n",
      "feature_fraction, val_score: 0.886879: 100%|##########| 7/7 [03:26<00:00, 29.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.886879:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.320738 seconds, init for row-wise cost 1.276004 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.499730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.886879:   5%|5         | 1/20 [00:27<08:34, 27.09s/it][I 2023-07-01 00:32:41,142] Trial 7 finished with value: 0.8868785101089008 and parameters: {'num_leaves': 41}. Best is trial 7 with value: 0.8868785101089008.\n",
      "num_leaves, val_score: 0.886879:   5%|5         | 1/20 [00:27<08:34, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.886879\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.187930 seconds, init for row-wise cost 1.214906 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.443127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  10%|#         | 2/20 [00:55<08:16, 27.60s/it][I 2023-07-01 00:33:09,051] Trial 8 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 85}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  10%|#         | 2/20 [00:55<08:16, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 85 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.265535 seconds, init for row-wise cost 1.290246 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.490403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  15%|#5        | 3/20 [01:28<08:34, 30.24s/it][I 2023-07-01 00:33:42,480] Trial 9 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 240}. Best is trial 8 with value: 0.8878170342950084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 240 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "num_leaves, val_score: 0.887817:  15%|#5        | 3/20 [01:28<08:34, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.254452 seconds, init for row-wise cost 1.320406 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.515747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  20%|##        | 4/20 [01:58<07:59, 29.98s/it][I 2023-07-01 00:34:12,085] Trial 10 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 95}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  20%|##        | 4/20 [01:58<07:59, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 95 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.318274 seconds, init for row-wise cost 1.268384 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.466621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  25%|##5       | 5/20 [02:31<07:49, 31.29s/it][I 2023-07-01 00:34:45,630] Trial 11 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 254}. Best is trial 8 with value: 0.8878170342950084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 254 and depth = 20\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "num_leaves, val_score: 0.887817:  25%|##5       | 5/20 [02:31<07:49, 31.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.270297 seconds, init for row-wise cost 1.276026 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.473774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 21\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 22\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  30%|###       | 6/20 [03:07<07:37, 32.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 225 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:35:21,143] Trial 12 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 225}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  30%|###       | 6/20 [03:07<07:37, 32.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.264841 seconds, init for row-wise cost 1.285757 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.532515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  35%|###5      | 7/20 [03:39<07:02, 32.50s/it][I 2023-07-01 00:35:53,218] Trial 13 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 148}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  35%|###5      | 7/20 [03:39<07:02, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 148 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.295221 seconds, init for row-wise cost 1.258665 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.664261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  40%|####      | 8/20 [04:10<06:24, 32.01s/it][I 2023-07-01 00:36:24,192] Trial 14 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 138}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  40%|####      | 8/20 [04:10<06:24, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 138 and depth = 18\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.193132 seconds, init for row-wise cost 1.153360 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.353316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  45%|####5     | 9/20 [04:33<05:23, 29.39s/it][I 2023-07-01 00:36:47,748] Trial 15 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 9}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  45%|####5     | 9/20 [04:33<05:23, 29.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 9 and depth = 7\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.304236 seconds, init for row-wise cost 1.293321 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.512491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  50%|#####     | 10/20 [05:06<05:04, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 192 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:37:20,766] Trial 16 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 192}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  50%|#####     | 10/20 [05:06<05:04, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.308798 seconds, init for row-wise cost 1.400015 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.608521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  55%|#####5    | 11/20 [05:36<04:33, 30.36s/it][I 2023-07-01 00:37:50,869] Trial 17 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 91}. Best is trial 8 with value: 0.8878170342950084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 91 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "num_leaves, val_score: 0.887817:  55%|#####5    | 11/20 [05:36<04:33, 30.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.259618 seconds, init for row-wise cost 1.249554 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.462788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  60%|######    | 12/20 [06:05<03:59, 29.98s/it][I 2023-07-01 00:38:19,891] Trial 18 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 70}. Best is trial 8 with value: 0.8878170342950084.\n",
      "num_leaves, val_score: 0.887817:  60%|######    | 12/20 [06:05<03:59, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 70 and depth = 10\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.293015 seconds, init for row-wise cost 1.375646 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.605000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.887817:  65%|######5   | 13/20 [06:37<03:33, 30.49s/it][I 2023-07-01 00:38:51,578] Trial 19 finished with value: 0.8878170342950084 and parameters: {'num_leaves': 176}. Best is trial 8 with value: 0.8878170342950084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 176 and depth = 19\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.887817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "num_leaves, val_score: 0.887817:  65%|######5   | 13/20 [06:37<03:33, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.250577 seconds, init for row-wise cost 1.396682 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.627765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  70%|#######   | 14/20 [07:07<03:01, 30.32s/it][I 2023-07-01 00:39:21,515] Trial 20 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 115}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636:  70%|#######   | 14/20 [07:07<03:01, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.298659 seconds, init for row-wise cost 1.264371 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.499139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  75%|#######5  | 15/20 [07:37<02:31, 30.22s/it][I 2023-07-01 00:39:51,470] Trial 21 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 113}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636:  75%|#######5  | 15/20 [07:37<02:31, 30.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 113 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.183241 seconds, init for row-wise cost 1.184992 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.386765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  80%|########  | 16/20 [08:06<01:59, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 107 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:40:20,445] Trial 22 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 107}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636:  80%|########  | 16/20 [08:06<01:59, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.270784 seconds, init for row-wise cost 1.202278 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.398017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  85%|########5 | 17/20 [08:36<01:29, 29.92s/it][I 2023-07-01 00:40:50,557] Trial 23 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 121}. Best is trial 20 with value: 0.8886359227161271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 121 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "num_leaves, val_score: 0.888636:  85%|########5 | 17/20 [08:36<01:29, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.293808 seconds, init for row-wise cost 1.221756 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.444035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  90%|######### | 18/20 [09:07<01:00, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 119 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:41:21,452] Trial 24 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 119}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636:  90%|######### | 18/20 [09:07<01:00, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.252499 seconds, init for row-wise cost 1.210295 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.416085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636:  95%|#########5| 19/20 [09:38<00:30, 30.49s/it][I 2023-07-01 00:41:52,497] Trial 25 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 165}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636:  95%|#########5| 19/20 [09:38<00:30, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 165 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.298884 seconds, init for row-wise cost 1.342315 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.583213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.888636: 100%|##########| 20/20 [10:06<00:00, 29.68s/it][I 2023-07-01 00:42:20,334] Trial 26 finished with value: 0.8886359227161271 and parameters: {'num_leaves': 57}. Best is trial 20 with value: 0.8886359227161271.\n",
      "num_leaves, val_score: 0.888636: 100%|##########| 20/20 [10:06<00:00, 30.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 57 and depth = 10\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.283088 seconds, init for row-wise cost 1.285526 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.498970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 86408 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 86278 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 86313 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 86070 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 85930 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  10%|#         | 1/10 [00:28<04:12, 28.02s/it][I 2023-07-01 00:42:48,490] Trial 27 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.5744836697466591, 'bagging_freq': 4}. Best is trial 27 with value: 0.8886359227161271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "bagging, val_score: 0.888636:  10%|#         | 1/10 [00:28<04:12, 28.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.229269 seconds, init for row-wise cost 1.190130 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.399313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 138704 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 138630 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 138766 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  20%|##        | 2/10 [00:57<03:49, 28.66s/it][I 2023-07-01 00:43:17,531] Trial 28 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.9249011767937809, 'bagging_freq': 7}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  20%|##        | 2/10 [00:57<03:49, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.232074 seconds, init for row-wise cost 0.942917 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.122883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 60898 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 61085 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 61044 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 60808 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 60883 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 60794 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 60817 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 60883 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 61037 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 60698 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 60957 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 60926 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 60993 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 60995 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 61115 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 60824 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 60650 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 60832 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 61085 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 61345 data to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  30%|###       | 3/10 [01:24<03:15, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:43:44,678] Trial 29 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.40598120770555474, 'bagging_freq': 1}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  30%|###       | 3/10 [01:24<03:15, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.293514 seconds, init for row-wise cost 1.302760 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.520678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 147873 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 147752 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 147852 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  40%|####      | 4/10 [01:54<02:52, 28.80s/it][I 2023-07-01 00:44:14,788] Trial 30 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.9854164725283208, 'bagging_freq': 7}. Best is trial 27 with value: 0.8886359227161271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "bagging, val_score: 0.888636:  40%|####      | 4/10 [01:54<02:52, 28.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.291100 seconds, init for row-wise cost 1.205591 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.412965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 110876 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 110834 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 110852 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 110822 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 110525 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 110735 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 110660 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 110724 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 110996 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 110619 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 110946 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 110280 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 110716 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 110639 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 111013 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 110477 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 110646 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 110994 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Re-bagging, using 111084 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 111139 data to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  50%|#####     | 5/10 [02:24<02:25, 29.20s/it][I 2023-07-01 00:44:44,605] Trial 31 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.7383935125389781, 'bagging_freq': 1}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  50%|#####     | 5/10 [02:24<02:25, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.332021 seconds, init for row-wise cost 1.554281 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.784825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 112387 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 112357 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 112363 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 112397 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 112038 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  60%|######    | 6/10 [02:59<02:05, 31.32s/it][I 2023-07-01 00:45:20,050] Trial 32 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.7485109033821102, 'bagging_freq': 4}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  60%|######    | 6/10 [02:59<02:05, 31.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.242562 seconds, init for row-wise cost 1.224846 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.445433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 65520 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 65568 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 65591 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 65398 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  70%|#######   | 7/10 [03:27<01:30, 30.29s/it][I 2023-07-01 00:45:48,267] Trial 33 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.4362958755853858, 'bagging_freq': 5}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  70%|#######   | 7/10 [03:27<01:30, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.408979 seconds, init for row-wise cost 1.260893 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.472285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 124740 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 124633 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 124607 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 124572 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 124216 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 124623 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 124613 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  80%|########  | 8/10 [03:57<01:00, 30.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:46:17,914] Trial 34 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.8307153689091664, 'bagging_freq': 3}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  80%|########  | 8/10 [03:57<01:00, 30.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.316378 seconds, init for row-wise cost 1.295765 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.522527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 88672 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 88558 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 88579 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 88345 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 88187 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 88338 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 88243 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 88475 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 88582 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 88155 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636:  90%|######### | 9/10 [04:24<00:29, 29.29s/it][I 2023-07-01 00:46:45,389] Trial 35 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.5896355033208046, 'bagging_freq': 2}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636:  90%|######### | 9/10 [04:25<00:29, 29.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.248286 seconds, init for row-wise cost 1.217235 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.413456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Re-bagging, using 128428 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 128254 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 128281 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 128364 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.888636: 100%|##########| 10/10 [04:53<00:00, 29.12s/it][I 2023-07-01 00:47:14,190] Trial 36 finished with value: 0.8886359227161271 and parameters: {'bagging_fraction': 0.8552728357052677, 'bagging_freq': 6}. Best is trial 27 with value: 0.8886359227161271.\n",
      "bagging, val_score: 0.888636: 100%|##########| 10/10 [04:53<00:00, 29.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.888636:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.281887 seconds, init for row-wise cost 1.264199 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.454470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636:  17%|#6        | 1/6 [00:30<02:30, 30.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:47:44,406] Trial 37 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636:  17%|#6        | 1/6 [00:30<02:30, 30.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.272871 seconds, init for row-wise cost 1.284446 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.495381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636:  33%|###3      | 2/6 [01:00<02:00, 30.03s/it][I 2023-07-01 00:48:14,311] Trial 38 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636:  33%|###3      | 2/6 [01:00<02:00, 30.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.274545 seconds, init for row-wise cost 1.304708 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.519732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636:  50%|#####     | 3/6 [01:29<01:28, 29.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:48:43,518] Trial 39 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636:  50%|#####     | 3/6 [01:29<01:28, 29.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.289327 seconds, init for row-wise cost 1.337704 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.556985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636:  67%|######6   | 4/6 [02:00<01:00, 30.48s/it][I 2023-07-01 00:49:15,293] Trial 40 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.88}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636:  67%|######6   | 4/6 [02:01<01:00, 30.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.247465 seconds, init for row-wise cost 1.428858 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.646072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636:  83%|########3 | 5/6 [02:31<00:30, 30.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:49:45,400] Trial 41 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636:  83%|########3 | 5/6 [02:31<00:30, 30.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.247025 seconds, init for row-wise cost 1.226112 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.458614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.888636: 100%|##########| 6/6 [03:00<00:00, 29.96s/it][I 2023-07-01 00:50:14,556] Trial 42 finished with value: 0.8886359227161271 and parameters: {'feature_fraction': 0.784}. Best is trial 37 with value: 0.8886359227161271.\n",
      "feature_fraction_stage2, val_score: 0.888636: 100%|##########| 6/6 [03:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.888636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.888636:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.184440 seconds, init for row-wise cost 1.123351 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.327840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:   5%|5         | 1/20 [00:29<09:11, 29.03s/it][I 2023-07-01 00:50:43,700] Trial 43 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 3.3866509133357354, 'lambda_l2': 1.4041138109033385e-07}. Best is trial 43 with value: 0.889444647250268.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "regularization_factors, val_score: 0.889445:   5%|5         | 1/20 [00:29<09:11, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.187974 seconds, init for row-wise cost 1.154563 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.349928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  10%|#         | 2/20 [00:57<08:38, 28.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:51:12,418] Trial 44 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 2.882222545511611, 'lambda_l2': 3.528686937325463e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  10%|#         | 2/20 [00:57<08:38, 28.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.273501 seconds, init for row-wise cost 1.282317 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.521777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  15%|#5        | 3/20 [01:26<08:13, 29.03s/it][I 2023-07-01 00:51:41,611] Trial 45 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.411603187060055, 'lambda_l2': 2.243083286884319e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  15%|#5        | 3/20 [01:27<08:13, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.237575 seconds, init for row-wise cost 1.391373 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.598608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  20%|##        | 4/20 [01:55<07:38, 28.63s/it][I 2023-07-01 00:52:09,658] Trial 46 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 7.702828584281146, 'lambda_l2': 2.362838618150639e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  20%|##        | 4/20 [01:55<07:38, 28.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.220299 seconds, init for row-wise cost 1.483555 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.700050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  25%|##5       | 5/20 [02:23<07:06, 28.45s/it][I 2023-07-01 00:52:37,796] Trial 47 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 7.718905853181482, 'lambda_l2': 1.9140216217805077e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  25%|##5       | 5/20 [02:23<07:06, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.202405 seconds, init for row-wise cost 1.074365 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.275705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 22\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  30%|###       | 6/20 [02:50<06:34, 28.16s/it][I 2023-07-01 00:53:05,332] Trial 48 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 7.559315145794619, 'lambda_l2': 1.9596843019723708e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  30%|###       | 6/20 [02:50<06:34, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.234244 seconds, init for row-wise cost 1.277636 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.522672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  35%|###5      | 7/20 [03:19<06:10, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:53:34,699] Trial 49 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.485163623815675, 'lambda_l2': 1.5887660611883522e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  35%|###5      | 7/20 [03:20<06:10, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.279077 seconds, init for row-wise cost 1.336913 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.546433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  40%|####      | 8/20 [03:48<05:43, 28.61s/it][I 2023-07-01 00:54:03,435] Trial 50 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 5.462946038938918, 'lambda_l2': 2.316130320344606e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  40%|####      | 8/20 [03:48<05:43, 28.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.228539 seconds, init for row-wise cost 1.134941 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.370963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  45%|####5     | 9/20 [04:16<05:13, 28.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:54:31,674] Trial 51 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.70096133648986, 'lambda_l2': 2.1913840252179594e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  45%|####5     | 9/20 [04:17<05:13, 28.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.191081 seconds, init for row-wise cost 1.229027 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.423246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  50%|#####     | 10/20 [04:44<04:43, 28.31s/it][I 2023-07-01 00:54:59,523] Trial 52 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.832375401436067, 'lambda_l2': 1.9282702945752386e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  50%|#####     | 10/20 [04:44<04:43, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.187738 seconds, init for row-wise cost 0.986111 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.164019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  55%|#####5    | 11/20 [05:13<04:15, 28.39s/it][I 2023-07-01 00:55:28,161] Trial 53 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.264039956026412, 'lambda_l2': 3.417792713023881e-08}. Best is trial 43 with value: 0.889444647250268.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "regularization_factors, val_score: 0.889445:  55%|#####5    | 11/20 [05:13<04:15, 28.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.302501 seconds, init for row-wise cost 1.286868 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.529243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  60%|######    | 12/20 [05:41<03:47, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:55:56,659] Trial 54 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 6.330111805534285, 'lambda_l2': 3.580013626566446e-08}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  60%|######    | 12/20 [05:42<03:47, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.257988 seconds, init for row-wise cost 1.449387 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.670741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  65%|######5   | 13/20 [06:12<03:24, 29.15s/it][I 2023-07-01 00:56:27,402] Trial 55 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 0.06506700299247777, 'lambda_l2': 5.976031752049182e-07}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  65%|######5   | 13/20 [06:12<03:24, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.203711 seconds, init for row-wise cost 1.443112 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.670262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  70%|#######   | 14/20 [06:41<02:54, 29.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:56:56,596] Trial 56 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 2.1102738922533952e-05, 'lambda_l2': 3.032710674212285e-06}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  70%|#######   | 14/20 [06:42<02:54, 29.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.183924 seconds, init for row-wise cost 0.984314 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.221138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  75%|#######5  | 15/20 [07:11<02:26, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:57:26,509] Trial 57 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 0.19518959511578693, 'lambda_l2': 0.0031940404064721606}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  75%|#######5  | 15/20 [07:11<02:26, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.235206 seconds, init for row-wise cost 1.210777 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.405199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.889445:  80%|########  | 16/20 [07:41<01:58, 29.52s/it][I 2023-07-01 00:57:56,287] Trial 58 finished with value: 0.889444647250268 and parameters: {'lambda_l1': 0.36451490119965246, 'lambda_l2': 5.588126995221761e-07}. Best is trial 43 with value: 0.889444647250268.\n",
      "regularization_factors, val_score: 0.889445:  80%|########  | 16/20 [07:41<01:58, 29.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.889445\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.219400 seconds, init for row-wise cost 1.238735 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.441030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.890067:  85%|########5 | 17/20 [08:11<01:29, 29.72s/it][I 2023-07-01 00:58:26,481] Trial 59 finished with value: 0.8900674850553022 and parameters: {'lambda_l1': 0.30296276025792374, 'lambda_l2': 2.492286902249665}. Best is trial 59 with value: 0.8900674850553022.\n",
      "regularization_factors, val_score: 0.890067:  85%|########5 | 17/20 [08:11<01:29, 29.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890067\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.246169 seconds, init for row-wise cost 1.263639 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.476086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.890298:  90%|######### | 18/20 [08:42<00:59, 29.88s/it][I 2023-07-01 00:58:56,803] Trial 60 finished with value: 0.8902981172435277 and parameters: {'lambda_l1': 0.2817999473750504, 'lambda_l2': 3.719000841940739}. Best is trial 60 with value: 0.8902981172435277.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "regularization_factors, val_score: 0.890298:  90%|######### | 18/20 [08:42<00:59, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.266351 seconds, init for row-wise cost 1.322136 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.527824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.890298:  95%|#########5| 19/20 [09:13<00:30, 30.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 00:59:27,891] Trial 61 finished with value: 0.8902981172435277 and parameters: {'lambda_l1': 0.3082613206369492, 'lambda_l2': 2.2942897295448046}. Best is trial 60 with value: 0.8902981172435277.\n",
      "regularization_factors, val_score: 0.890298:  95%|#########5| 19/20 [09:13<00:30, 30.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.284138 seconds, init for row-wise cost 1.257302 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.478080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.890298: 100%|##########| 20/20 [09:43<00:00, 30.29s/it][I 2023-07-01 00:59:58,209] Trial 62 finished with value: 0.8902981172435277 and parameters: {'lambda_l1': 0.16422158193277708, 'lambda_l2': 3.875295933871404}. Best is trial 60 with value: 0.8902981172435277.\n",
      "regularization_factors, val_score: 0.890298: 100%|##########| 20/20 [09:43<00:00, 29.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.890298:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.298447 seconds, init for row-wise cost 1.415457 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.662516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.890889:  20%|##        | 1/5 [00:30<02:00, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 01:00:28,512] Trial 63 finished with value: 0.8908887747013381 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.8908887747013381.\n",
      "min_data_in_leaf, val_score: 0.890889:  20%|##        | 1/5 [00:30<02:00, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.331158 seconds, init for row-wise cost 1.272321 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.474083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.890889:  40%|####      | 2/5 [01:01<01:31, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 01:00:59,471] Trial 64 finished with value: 0.8908887747013381 and parameters: {'min_child_samples': 10}. Best is trial 63 with value: 0.8908887747013381.\n",
      "min_data_in_leaf, val_score: 0.890889:  40%|####      | 2/5 [01:01<01:31, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.299890 seconds, init for row-wise cost 1.402217 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.606142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 22\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.890889:  60%|######    | 3/5 [01:33<01:02, 31.26s/it][I 2023-07-01 01:01:31,402] Trial 65 finished with value: 0.8908887747013381 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.8908887747013381.\n",
      "min_data_in_leaf, val_score: 0.890889:  60%|######    | 3/5 [01:33<01:02, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.890889\n",
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 1.211118 seconds, init for row-wise cost 1.739011 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.007389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.891102:  80%|########  | 4/5 [36:56<14:17, 857.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.891102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 01:36:54,631] Trial 66 finished with value: 0.8911021565714513 and parameters: {'min_child_samples': 50}. Best is trial 66 with value: 0.8911021565714513.\n",
      "min_data_in_leaf, val_score: 0.891102:  80%|########  | 4/5 [36:56<14:17, 857.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9586, number of negative: 140414\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.880361\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.515813\n",
      "[LightGBM] [Debug] init for col-wise cost 0.451700 seconds, init for row-wise cost 1.470046 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.713202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 123224\n",
      "[LightGBM] [Info] Number of data points in the train set: 150000, number of used features: 784\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063907 -> initscore=-2.684292\n",
      "[LightGBM] [Info] Start training from score -2.684292\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.891102: 100%|##########| 5/5 [37:40<00:00, 563.94s/it][I 2023-07-01 01:37:38,715] Trial 67 finished with value: 0.8911021565714513 and parameters: {'min_child_samples': 100}. Best is trial 66 with value: 0.8911021565714513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 115 and depth = 15\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.891102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.891102: 100%|##########| 5/5 [37:40<00:00, 452.10s/it]\n"
     ]
    }
   ],
   "source": [
    "test, train = split_test_train(df_combine)\n",
    "\n",
    "y_train = train['target'].round(0).astype(int)\n",
    "y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "x_train = train.drop(['target'], axis=1)\n",
    "x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "#Specifying the parameter\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_test = lgb.Dataset(x_test, label=y_test, reference=d_train)\n",
    "\n",
    "params={}\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='binary' #Binary target feature\n",
    "params['metric']='auc' #metric for binary classification\n",
    "#params['num_boost_round']=100\n",
    "#params['early_stopping_rounds']=30\n",
    "#params['max_depth']=100\n",
    "#params['is_unbalance']=True\n",
    "params['verbosity'] = 2\n",
    "\n",
    "#train the model \n",
    "clf=lgb.train(params = params, \n",
    "              train_set = d_train,\n",
    "              valid_sets = d_test, \n",
    "              num_boost_round = 20, \n",
    "              callbacks= [lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "#prediction on the test set\n",
    "predicted=clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fd998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a60afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8911021565714514"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metrics.roc_auc(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d521ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e366e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0237755 , 0.01348663, 0.01358502, ..., 0.0397863 , 0.04060678,\n",
       "       0.59270895])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32ff72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e76a99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "924f68e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224238253867485"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metrics.roc_auc(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1c46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df0646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dadace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f24909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032ebdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae6e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd248bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c43f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370d7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b107f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -> 0.5\n",
      "4 -> 0.5\n",
      "5 -> 0.5000247244095933\n",
      "6 -> 0.49999288747270565\n",
      "7 -> 0.49999288747270565\n",
      "8 -> 0.562710535166727\n",
      "9 -> 0.562710535166727\n",
      "10 -> 0.5629195075422255\n",
      "11 -> 0.5748062123180953\n",
      "12 -> 0.575639392736126\n",
      "13 -> 0.575575549426414\n",
      "14 -> 0.5756463358274835\n",
      "15 -> 0.5754974810621133\n",
      "16 -> 0.5757315167190786\n",
      "17 -> 0.5737078370499589\n",
      "18 -> 0.5737078370499589\n",
      "19 -> 0.5744663359313992\n",
      "20 -> 0.5789362128999035\n",
      "21 -> 0.5808645511942971\n",
      "22 -> 0.5804355987804852\n",
      "23 -> 0.5770969506326258\n",
      "24 -> 0.5760161872467884\n",
      "25 -> 0.5779761930421329\n",
      "26 -> 0.5780649301973749\n",
      "27 -> 0.5765264254166746\n",
      "28 -> 0.5765264254166746\n",
      "29 -> 0.5782031161718039\n",
      "30 -> 0.5787951484055748\n",
      "31 -> 0.5786925250676175\n",
      "32 -> 0.5782246231896236\n",
      "33 -> 0.5788908286521744\n",
      "34 -> 0.5775121862996596\n",
      "35 -> 0.5775121862996596\n",
      "36 -> 0.5768918733926481\n",
      "37 -> 0.5787846490505701\n",
      "38 -> 0.5816553953204356\n",
      "39 -> 0.581336178771875\n",
      "40 -> 0.5796738825072713\n",
      "41 -> 0.5792945483485827\n",
      "42 -> 0.5801063911847305\n",
      "43 -> 0.5796456018340308\n",
      "44 -> 0.5823569939835213\n",
      "45 -> 0.5823569939835213\n",
      "46 -> 0.5797130014073901\n",
      "47 -> 0.5834839887967718\n",
      "48 -> 0.583590337834313\n",
      "49 -> 0.5813647983169892\n",
      "50 -> 0.5813647983169892\n",
      "51 -> 0.5807480416736249\n",
      "52 -> 0.5849092014485734\n",
      "53 -> 0.5849092014485734\n",
      "54 -> 0.5843032830320876\n",
      "55 -> 0.584253664776964\n",
      "56 -> 0.5843811819604514\n",
      "57 -> 0.584966101666928\n",
      "58 -> 0.5844839747343454\n",
      "59 -> 0.585408940263455\n",
      "60 -> 0.585430277845338\n",
      "61 -> 0.5863694684290364\n",
      "62 -> 0.5868053639342058\n",
      "63 -> 0.5860505907523494\n",
      "64 -> 0.5860505907523494\n",
      "65 -> 0.5860505907523494\n",
      "66 -> 0.5860505907523494\n",
      "67 -> 0.5860080850245204\n",
      "68 -> 0.5856499190117779\n",
      "69 -> 0.5874255074052804\n",
      "70 -> 0.587616698462543\n",
      "71 -> 0.5882053438686037\n",
      "72 -> 0.5888469943574982\n",
      "73 -> 0.5892437709625489\n",
      "74 -> 0.589332508117791\n",
      "75 -> 0.5893360643814383\n",
      "76 -> 0.5894566384735682\n",
      "77 -> 0.5880989948310625\n",
      "78 -> 0.588822100511968\n",
      "79 -> 0.5881662249684849\n",
      "80 -> 0.5887547009386086\n",
      "81 -> 0.5886234580555373\n",
      "82 -> 0.5886236274914742\n",
      "83 -> 0.5885491153908207\n",
      "84 -> 0.5885491153908207\n",
      "85 -> 0.5889425051681609\n",
      "86 -> 0.5889425051681609\n",
      "87 -> 0.5898637449976868\n",
      "88 -> 0.5908489975728612\n",
      "89 -> 0.5890408944987238\n",
      "90 -> 0.5907390922716729\n",
      "91 -> 0.5910901457571209\n",
      "92 -> 0.5912139372410243\n",
      "93 -> 0.5906183487436061\n",
      "94 -> 0.5898491810712243\n",
      "95 -> 0.5911221521299455\n",
      "96 -> 0.5913844684601512\n",
      "97 -> 0.5892825509907941\n",
      "98 -> 0.5905409581230527\n",
      "99 -> 0.5911577147664171\n",
      "100 -> 0.5904131020676917\n",
      "101 -> 0.5904131020676917\n",
      "102 -> 0.5907038685070749\n",
      "103 -> 0.5916044484984647\n",
      "104 -> 0.5920367877399871\n",
      "105 -> 0.5927065494661853\n",
      "106 -> 0.5934296551470907\n",
      "107 -> 0.5932843566453676\n",
      "108 -> 0.5932843566453676\n",
      "109 -> 0.5932843566453676\n",
      "110 -> 0.5938550512972556\n",
      "111 -> 0.5940357429995136\n",
      "112 -> 0.5939896810080373\n",
      "113 -> 0.5935392215764055\n",
      "114 -> 0.5927773358672548\n",
      "115 -> 0.594265883520958\n",
      "116 -> 0.593978842781159\n",
      "117 -> 0.593978842781159\n",
      "118 -> 0.593978842781159\n",
      "119 -> 0.5935251659577537\n",
      "120 -> 0.5935251659577537\n",
      "121 -> 0.5938797757068489\n",
      "122 -> 0.5937520890874247\n",
      "123 -> 0.5935783404765241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m regressor \u001b[38;5;241m=\u001b[39m lg\u001b[38;5;241m.\u001b[39mLGBMClassifier()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# calculate metric\u001b[39;00m\n\u001b[0;32m     25\u001b[0m predicted \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mpredict(test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "existing = ['target', 'ind']\n",
    "roc_auc = -1\n",
    "prev_roc_auc = -1\n",
    "increase_rate = -1\n",
    "\n",
    "for c in list(df_combine.columns):\n",
    "    if c != 'target':\n",
    "        \n",
    "        # add column\n",
    "        existing.append(c)\n",
    "        \n",
    "        # new dataset\n",
    "        df = df_combine[existing]\n",
    "        \n",
    "        # build model\n",
    "        test, train = split_test_train(df)\n",
    "        y_train = train['target']\n",
    "        y_test = test['target']\n",
    "        train = train.drop(['target'], axis=1)\n",
    "        test = test.drop(['target'], axis=1)\n",
    "        regressor = lg.LGBMClassifier()\n",
    "        regressor.fit(train, y_train)\n",
    "        \n",
    "        # calculate metric\n",
    "        predicted = regressor.predict(test)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = Metrics.roc_auc(y_test, predicted)\n",
    "        except ValueError:\n",
    "            roc_auc = -1\n",
    "        \n",
    "        print(f'{len(existing)} -> {roc_auc}')\n",
    "        \n",
    "        # remove current column\n",
    "        # existing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5219b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3553982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390f926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924fee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6553461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd98914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b3fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648debd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f24ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3265607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc4897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a109e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1d04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073aba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576d10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53ad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2832380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4ab493d",
   "metadata": {},
   "source": [
    "### Check if dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned = len(df_combine[df_combine['target'] == 1])\n",
    "not_churned = len(df_combine[df_combine['target'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [churned, not_churned]\n",
    "x = ['Churned', 'Not churned']\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd1142",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "#### Check inbound calls from non-vodaphone number\n",
    "#### Check outbound calls to non-vodaphone number\n",
    "#### SMS from non-vodaphone number\n",
    "#### SMS to non-vodaphone number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47767b50",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c23854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b386d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366986a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8b4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee8d68b",
   "metadata": {},
   "source": [
    "## Explore numbers abonent had communication with + frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ce76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned = df_combine[df_combine['target'] == 1]\n",
    "churned_with_nums = pd.merge(churned, df_combine_num, on='abon_id', how='left')\n",
    "churned_numbers = list(churned_with_nums['bnum'].unique())\n",
    "\n",
    "non_churned = df_combine[df_combine['target'] == 0]\n",
    "non_churned_with_nums = pd.merge(non_churned, df_combine_num, on='abon_id', how='left')\n",
    "non_churned_numbers = list(non_churned_with_nums['bnum'].unique())\n",
    "\n",
    "number_abon_had_communicated = (set(churned_numbers) - set(non_churned_numbers))\n",
    "df_number_abon_had_communicated = pd.DataFrame(number_abon_had_communicated, columns= ['bnum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fddd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned_with_nums[churned_with_nums['bnum'].isin(list(number_abon_had_communicated))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2d077",
   "metadata": {},
   "source": [
    "## Telephone Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75958788",
   "metadata": {},
   "source": [
    "## Groupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = train_num.groupby(['abon_id'])\n",
    "gr.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6992251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_matrix = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6700b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facebe50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad075db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c644d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9765e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d167d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e5976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbbe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9276c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
