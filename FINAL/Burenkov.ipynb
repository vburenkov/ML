{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a29a5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lg\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "import string \n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "from typing import List, TypeVar, Dict, Callable\n",
    "import abc\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, precision_score, average_precision_score, roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from numpy import argmax\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8636c9",
   "metadata": {},
   "source": [
    "0) use predictproba instead of predict +\n",
    "1) fix train set +\n",
    "2) calculate threshold (precision, recall, f1) +\n",
    "3) probability distribution\n",
    "4) Lecture 6.1, 6.0.1\n",
    "5) SHAP\n",
    "6) Exploratory analysis\n",
    "7) Feature engineering\n",
    "\n",
    "====\n",
    "\n",
    "15 - 400\n",
    "23 - 150\n",
    "50 - 0.895\n",
    "\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78db6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_СORRELATION_MATRIX_PATH_ = 'corr_matrix.pcl'\n",
    "_СHURN_PATH_ = 'churn_model.pcl'\n",
    "_CHURN_PATH_REDUCED_ = 'churn_model_reduced.pcl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3885d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077\n",
    "# https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "# https://www.statology.org/plot-roc-curve-python/\n",
    "# https://analyticsindiamag.com/complete-guide-to-lightgbm-boosting-algorithm-in-python/\n",
    "class Metrics:\n",
    "    \n",
    "    def roc_auc(y_true, predicted):\n",
    "        return roc_auc_score(y_true, predicted)\n",
    "    \n",
    "    def plot_auc(y_true:list, predicted:list, labels:list):\n",
    "        \n",
    "        for i in range(0, len(y_true)):\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_true[i], predicted[i])\n",
    "            auc = metrics.roc_auc_score(y_true[i], predicted[i])\n",
    "            plt.plot(fpr,tpr, label=f\"{labels[i]} AUC=\"+str(auc))\n",
    "\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "    \n",
    "    def XY_plot(Xs:list, Ys:list, legends:list, x_label:str, y_label:str):\n",
    "        \n",
    "        for i in range(0, len(Xs)):      \n",
    "            plt.plot(Xs[i], Ys[i], label=legends[i])\n",
    "        \n",
    "        plt.ylabel(y_label)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "    \n",
    "    def BAR_plot(X, Y, title:str, x_label:str, y_label:str):\n",
    "        fig = plt.figure(figsize = (10, 5))\n",
    "        plt.bar(X, Y, color ='maroon', width = 0.4)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def classification_report(y_true, predicted):\n",
    "        return classification_report(y_true, predicted)\n",
    "    \n",
    "    def confusion_matrix(y_true, predicted):\n",
    "        return confusion_matrix(y_true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77db8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_string(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    result_str = ''.join(random.choice(letters) for i in range(length))\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1480700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_float_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=float).columns.tolist()\n",
    "\n",
    "def get_int_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=int).columns.tolist()\n",
    "\n",
    "def get_number_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(np.number).columns.tolist()\n",
    "\n",
    "def get_obj_cols(df:DataFrame) -> List[str]:\n",
    "    return list(df.select_dtypes(include=object).columns)\n",
    "    \n",
    "def get_empty_cols(df:DataFrame):\n",
    "    return list(df.isnull().sum()[lambda x: x > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1207571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True) -> pd.DataFrame:\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0ffcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(df:DataFrame):\n",
    "    test, train = df[df['ind'].eq('test')], df[df['ind'].eq('train')]\n",
    "    test = test.drop(['ind'], axis=1)\n",
    "    train = train.drop(['ind'], axis=1)\n",
    "    return test, train\n",
    "    \n",
    "def combine_test_train(test:DataFrame, train:DataFrame):\n",
    "    combine = pd.concat([test.assign(ind='test'), train.assign(ind='train')])\n",
    "    target = train['target']\n",
    "    test_ids = test['Id']\n",
    "    return combine, target, test_ids\n",
    "\n",
    "def combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi):\n",
    "    df_combine = pd.concat([train_df.assign(ind='train'), test_df.assign(ind='test')])\n",
    "    df_combine_num = pd.concat([train_num.assign(ind='train'), test_num.assign(ind='test')])\n",
    "    df_combine_dpi = pd.concat([train_dpi.assign(ind='train'), test_dpi.assign(ind='test')])\n",
    "    return df_combine, df_combine_num, df_combine_dpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65fd4a",
   "metadata": {},
   "source": [
    "## Data size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f25481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_churn_data():\n",
    "\n",
    "    if os.path.exists(_CHURN_PATH_REDUCED_) == False:\n",
    "        if os.path.exists(_СHURN_PATH_) == True: \n",
    "    \n",
    "            with open(_СHURN_PATH_, 'rb') as file:\n",
    "                deserialized_object = pickle.load(file)\n",
    "\n",
    "            deserialized_object = list(deserialized_object)\n",
    "        \n",
    "            # reduce size\n",
    "            train_df= reduce_mem_usage(deserialized_object[1][1])\n",
    "            train_num_reduced = reduce_mem_usage(deserialized_object[1][2])\n",
    "            train_dpi_reduced = reduce_mem_usage(deserialized_object[1][3])\n",
    "\n",
    "            test_df = reduce_mem_usage(deserialized_object[2][1])\n",
    "            test_num_reduced = reduce_mem_usage(deserialized_object[2][2])\n",
    "            test_dpi_reduced = reduce_mem_usage(deserialized_object[2][3])\n",
    "            \n",
    "            # dump data back\n",
    "            deserialized_object = tuple([[train_df, train_num_reduced, train_dpi_reduced], [test_df, test_num_reduced, test_dpi_reduced]])\n",
    "            pickle.dump(deserialized_object, open(_CHURN_PATH_REDUCED_, 'wb'))\n",
    "    else:\n",
    "        print(f'{_CHURN_PATH_REDUCED_} already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6e18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_data():\n",
    "\n",
    "    with open(_СHURN_PATH_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[1][1]\n",
    "    train_num = deserialized_object[1][2]\n",
    "    train_dpi = deserialized_object[1][3]\n",
    "\n",
    "    test_df = deserialized_object[2][1]\n",
    "    test_num = deserialized_object[2][2]\n",
    "    test_dpi = deserialized_object[2][3]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d952ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_reduced():\n",
    "\n",
    "    with open(_CHURN_PATH_REDUCED_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[0][0]\n",
    "    train_num = deserialized_object[0][1]\n",
    "    train_dpi = deserialized_object[0][2]\n",
    "\n",
    "    test_df = deserialized_object[1][0]\n",
    "    test_num = deserialized_object[1][1]\n",
    "    test_dpi = deserialized_object[1][2]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f00265aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelephoneHelper:\n",
    "    \n",
    "    def is_short_number(number:str) -> bool:\n",
    "        if (number.isdigit() and len(number) <= 4):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_life(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['063', '093', '073' ])):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_kyivstar(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['067', '097', '068', '098', '098'])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['050', '095', '099', '066' ])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_non_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and not TelephoneHelper.is_vodafone(number)):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_string_number(number:str) -> bool:\n",
    "        if (number.isdigit() == False):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca85d64",
   "metadata": {},
   "source": [
    "### Feature selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7024508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureHelper:\n",
    "    \n",
    "    def get_feature_correlation_df(corr_m, remove_duplicates=True, remove_self_correlations=True):\n",
    "    \n",
    "        corr_matrix_abs = corr_m.abs()\n",
    "        corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
    "        sorted_correlated_features = corr_matrix_abs_us \\\n",
    "            .sort_values(kind=\"quicksort\", ascending=False) \\\n",
    "            .reset_index()\n",
    "\n",
    "        # Remove comparisons of the same feature\n",
    "        if remove_self_correlations:\n",
    "            sorted_correlated_features = sorted_correlated_features[\n",
    "                (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
    "            ]\n",
    "\n",
    "        # Remove duplicates\n",
    "        if remove_duplicates:\n",
    "            sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
    "\n",
    "        # Create meaningful names for the columns\n",
    "        sorted_correlated_features.columns = ['f1', 'f2', 'corr']\n",
    "\n",
    "        return sorted_correlated_features\n",
    "    \n",
    "    def get_correlation_matrix(df:DataFrame, method:str, save_path:str):\n",
    "        if os.path.exists(save_path) == False:\n",
    "            corr_matrix = df.corr(method = method, numeric_only = True)\n",
    "            pickle.dump(corr_matrix, open(save_path, 'wb'))\n",
    "        else:\n",
    "            corr_matrix = pickle.load(open(save_path, 'rb'))\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def remove_aggr_function(str_to_check:str) -> str:\n",
    "        parts = str_to_check.split('_')\n",
    "        \n",
    "        if (len(parts) > 2):\n",
    "            index_to_remove = len(parts) - 2\n",
    "            \n",
    "            # remove aggregation function\n",
    "            if (parts[index_to_remove] in ['min', 'std', 'max', 'mea', 'td']):\n",
    "                parts.remove(parts[index_to_remove])\n",
    "                \n",
    "            result = '_'.join(parts)\n",
    "            return result\n",
    "        else:\n",
    "            return str_to_check    \n",
    "\n",
    "        \n",
    "    def get_heatmap_matrix(corr_matrix:DataFrame):\n",
    "        heatmap_matrix = pd.DataFrame(corr_matrix['target'].abs())\n",
    "        heatmap_matrix = heatmap_matrix.sort_values(by='target', ascending=False)\n",
    "        heatmap_matrix = heatmap_matrix.drop(index=['target'])           \n",
    "        return heatmap_matrix\n",
    "    \n",
    "    # index - column name\n",
    "    # target - value\n",
    "    def plot_heatmap(heatmap_matrix:DataFrame):\n",
    "        plt.figure(figsize=(40, 120))\n",
    "        heatmap = sns.heatmap(heatmap_matrix, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "        heatmap.set_title('Features Correlating with Churn Rate', fontdict={'fontsize':18}, pad=16);\n",
    "        return heatmap_matrix\n",
    "    \n",
    "    def get_important_features(heatmap_matrix:DataFrame, use_groupping = False, num_of_features:int = -1):\n",
    "        df_features = heatmap_matrix.reset_index()\n",
    "        df_features = df_features.rename(columns = {'index':'feature'})\n",
    "        \n",
    "        # apply aggregation function for further groupping\n",
    "        df_features['feature_group'] = df_features['feature'].apply(FeatureHelper.remove_aggr_function)\n",
    "        df_features = df_features[['feature', 'feature_group', 'target']]\n",
    "        sorted_features = df_features.sort_values(by=['feature_group', 'target'], ascending = [False, False])\n",
    "        \n",
    "        # take first item from the group\n",
    "        if (use_groupping == True):\n",
    "            important_features = sorted_features.groupby('feature_group').first()\n",
    "        else:\n",
    "            important_features = sorted_features\n",
    "\n",
    "        # order by target\n",
    "        important_features = important_features.sort_values(by='target', ascending=False)\n",
    "           \n",
    "        # take N first rows\n",
    "        if (num_of_features != -1):\n",
    "            important_features = important_features.head(num_of_features)\n",
    "        \n",
    "        # optimize for heatmap\n",
    "        important_features = important_features.reset_index()\n",
    "        important_features = important_features[['feature', 'target']]\n",
    "        important_features.index = important_features['feature']\n",
    "        important_features.index.name = None\n",
    "        important_features = important_features[['target']]\n",
    "        important_features = important_features[important_features['target'] > 0]\n",
    "        \n",
    "        return important_features\n",
    "    \n",
    "    def get_important_features_tuples(heatmap_matrix:DataFrame, num_of_features:int = -1):\n",
    "        important_features = FeatureHelper.get_important_features(heatmap_matrix, num_of_features)\n",
    "        \n",
    "        if (num_of_features == -1):\n",
    "            num_of_features = len(important_features)\n",
    "        \n",
    "        important_features_tuples = list(zip(important_features.index, \n",
    "                                             important_features.target, \n",
    "                                             list(range(0, num_of_features))))\n",
    "        \n",
    "        return important_features_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b170f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    # index - feature name, target\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    # 1 - feature name, 2 - target, 3 - sorted number\n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe1177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationSelector(FeatureSelector):\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, corr_method:str, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.corr_method = corr_method\n",
    "        self.file_prefix = corr_method\n",
    "        self.num_of_features = num_of_features\n",
    "        self.get_heatmap()\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        self.save_path = f'{self.file_prefix}_{_СORRELATION_MATRIX_PATH_}'\n",
    "        self.corr_m = FeatureHelper.get_correlation_matrix(self.data, self.corr_method, self.save_path)\n",
    "        self.heatmap_m = FeatureHelper.get_heatmap_matrix(self.corr_m)\n",
    "        return self.heatmap_m \n",
    "    \n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "    \n",
    "    def get_non_correlated_features(self, barrier_coef:float, do_log:bool) -> List[str]:\n",
    "        \n",
    "        important_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        # f1, f2, corr\n",
    "        features_corr = FeatureHelper.get_feature_correlation_df(self.corr_m)\n",
    "        \n",
    "        already_processed = set()\n",
    "        all_features = [t[0] for t in important_tuples]\n",
    "\n",
    "        for f in all_features:\n",
    "            # get correlated features\n",
    "            correlated = list(features_corr[(features_corr['f1']==f) & (features_corr['corr'] > barrier_coef)]['f2'])\n",
    "\n",
    "            # if highly correlated features exist\n",
    "            if (len(correlated)>0):\n",
    "\n",
    "                for to_remove in correlated:\n",
    "                    if (to_remove not in already_processed):\n",
    "                        if (to_remove in all_features):\n",
    "                            all_features.remove(to_remove)\n",
    "                            if do_log: print(f'Removing: {to_remove} for {f}')            \n",
    "\n",
    "        # remember initial feature\n",
    "        already_processed.add(f)\n",
    "        \n",
    "        # return non-correlated features\n",
    "        return all_features\n",
    "    \n",
    "    def get_important_noncorrelated_features_tuples(self, mutual_correlation:float, feature_importance:float):\n",
    "        \n",
    "        final = []\n",
    "        important = self.get_important_features_tuples()\n",
    "        non_correlated = self.get_non_correlated_features(mutual_correlation, False)\n",
    "        \n",
    "        for i in important:\n",
    "            if (i[0] in non_correlated and i[1] > feature_importance):\n",
    "                final.append(i)\n",
    "\n",
    "        return final\n",
    "                        \n",
    "    def get_important_features_tuples(self) ->  List[tuple[str, float, int]]:\n",
    "        return FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return type(self).__name__ + '_' + self.corr_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bd308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nitin9809.medium.com/lightgbm-binary-classification-multi-class-classification-regression-using-python-4f22032b36a2\n",
    "# https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n",
    "class LGBMSelector(FeatureSelector):\n",
    "    \n",
    "    model_file = '___LGBMSelector_simple.pcl'\n",
    "    heatmap_calculated = False\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.num_of_features = num_of_features\n",
    "        self.train_model()\n",
    "    \n",
    "    def train_model(self):\n",
    "        test, train = split_test_train(self.data)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        self.x_train = train.drop(['target'], axis=1)\n",
    "        self.x_test = test.drop(['target'], axis=1)\n",
    "        \n",
    "        # regressor\n",
    "        if os.path.exists(self.model_file) == True: \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "        else:         \n",
    "            self.regressor = lg.LGBMClassifier()\n",
    "            self.regressor.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        # predict\n",
    "        self.test_predicted = self.regressor.predict_proba(self.x_test)[:,1]\n",
    "        self.train_predicted = self.regressor.predict_proba(self.x_train)[:,1]\n",
    "        \n",
    "        #save model\n",
    "        pickle.dump(self.regressor, open(self.model_file, 'wb'))\n",
    "    \n",
    "    def get_feature_importance_raw(self):\n",
    "        return self.regressor.feature_importances_\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        df_feature_importance = pd.DataFrame(list(zip(list(self.get_feature_importance_raw()), list(self.data.columns))))\n",
    "        df_feature_importance = df_feature_importance.set_axis(['target', 'feature'], axis=1)\n",
    "        df_feature_importance = df_feature_importance.sort_values(by=['target'], ascending=False)\n",
    "        df_feature_importance = df_feature_importance.set_index('feature')\n",
    "        df_feature_importance.index.name = None\n",
    "        self.heatmap_m = df_feature_importance\n",
    "        self.heatmap_calculated = True\n",
    "        return self.heatmap_m\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        if (self.heatmap_calculated == False):\n",
    "            self.get_heatmap()\n",
    "            \n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        if (self.heatmap_calculated == False):\n",
    "            self.get_heatmap()\n",
    "            \n",
    "        features_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        return features_tuples\n",
    "        \n",
    "    def get_test_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_test, self.test_predicted)\n",
    "\n",
    "    def get_train_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_train, self.train_predicted)\n",
    "\n",
    "    def get_test_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_train, self.train_predicted)\n",
    "    \n",
    "    def get_test_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_train, self.train_predicted)\n",
    "\n",
    "    def __str__(self):\n",
    "        return type(self).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583ceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "class EnhancedLGBMSelector(LGBMSelector):\n",
    "    \n",
    "    def __init__(self, data, model_file_name:str = '___EnhancedLGBMSelector_model_800_features.pcl'):\n",
    "        self.model_file = model_file_name\n",
    "        super().__init__(data, -1)\n",
    "    \n",
    "    def get_feature_importance_raw(self):\n",
    "        return self.regressor.feature_importance()\n",
    "    \n",
    "    def train_model(self):        \n",
    "        \n",
    "        test, train = split_test_train(self.data)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        self.x_train = train.drop(['target'], axis=1)\n",
    "        self.x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "        X_train_final, X_test_validation, y_train_final, y_test_validation = train_test_split(self.x_train, \n",
    "                                                                                              self.y_train, \n",
    "                                                                                              stratify=self.y_train,\n",
    "                                                                                              test_size=0.25)\n",
    "        \n",
    "        # Specifying datasets\n",
    "        d_train = lg.Dataset(X_train_final, label=y_train_final)\n",
    "        d_test = lg.Dataset(X_test_validation, label=y_test_validation, reference=d_train)\n",
    "\n",
    "        # load model from disk\n",
    "        if os.path.exists(self.model_file) == True: \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "        # Train model\n",
    "        else:\n",
    "            params={}\n",
    "            params['boosting_type']='dart' \n",
    "            params['objective']='binary' \n",
    "            params['metric']='auc' \n",
    "            params['verbosity'] = 2\n",
    "            params['is_unbalance'] = True\n",
    "\n",
    "            #train the model \n",
    "            self.regressor=lg.train(params = params, \n",
    "                                    train_set = d_train,\n",
    "                                    valid_sets = d_test, \n",
    "                                    num_boost_round = 150, \n",
    "                                    callbacks= [lg.early_stopping(stopping_rounds=20)])\n",
    "        \n",
    "        # make prediction     \n",
    "        self.test_predicted = self.regressor.predict(self.x_test)\n",
    "        self.train_predicted = self.regressor.predict(self.x_train)\n",
    "        \n",
    "        #save model\n",
    "        if os.path.exists(self.model_file) == False:\n",
    "            print(f'Saving model to file {self.model_file}. Data shape: {self.data.shape}. Train ROCAUC: {self.get_train_ROCAUC()}. Test ROCAUC: {self.get_test_ROCAUC()}')\n",
    "            pickle.dump(self.regressor, open(self.model_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f771c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeLGBMSelector(FeatureSelector):\n",
    "    \n",
    "    results_file = 'IterativeLGBMSelector_results.pcl'\n",
    "    model_file = 'IterativeLGBMSelector_model.pcl'\n",
    "    df_file = 'IterativeLGBMSelector_df.pcl'\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data:pd.DataFrame, \n",
    "                 selector:EnhancedLGBMSelector,\n",
    "                 base_line_auc:float,\n",
    "                 num_of_boosting_rounds:int,\n",
    "                 do_log:bool):\n",
    "        \n",
    "        self.results_file = f'___{base_line_auc}_{num_of_boosting_rounds}_{self.results_file}'\n",
    "        self.model_file = f'___{base_line_auc}_{num_of_boosting_rounds}_{self.model_file}'\n",
    "        self.df_file =  f'___{base_line_auc}_{num_of_boosting_rounds}_{self.df_file}'\n",
    "        \n",
    "        self.data = data\n",
    "        self.trained_selector = selector\n",
    "        self.base_line_auc = base_line_auc\n",
    "        self.num_of_boosting_rounds = num_of_boosting_rounds\n",
    "        self.do_log = do_log\n",
    "        \n",
    "        # check pre-saved results\n",
    "        if os.path.exists(self.results_file) == True:\n",
    "            \n",
    "            with open(self.results_file, 'rb') as file:\n",
    "                self.result = pickle.load(file)\n",
    "            \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "                \n",
    "            with open(self.df_file, 'rb') as file:\n",
    "                self.df = pickle.load(file)\n",
    "            \n",
    "            # save data for metrics\n",
    "            test, train = split_test_train(self.df)\n",
    "            self.y_train = train['target'].round(0).astype(int)\n",
    "            self.y_test = test['target'].round(0).astype(int)\n",
    "            self.x_train = train.drop(['target'], axis=1)\n",
    "            self.x_test = test.drop(['target'], axis=1)\n",
    "            \n",
    "            # predict\n",
    "            self.train_predicted = self.regressor.predict(self.x_train)\n",
    "            self.test_predicted = self.regressor.predict(self.x_test)\n",
    "            \n",
    "        else:\n",
    "            self.result = self.train_model()\n",
    "    \n",
    "    def get_regressor(self):\n",
    "        return self.regressor\n",
    "    \n",
    "    def get_training_results(self):\n",
    "        return self.result\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        all_features = [f[0] for f in self.trained_selector.get_important_features_tuples()]\n",
    "        do_log = True\n",
    "        existing = ['target', 'ind']\n",
    "        roc_auc = 0\n",
    "        prev_roc_auc = 0\n",
    "        increase_rate = 0\n",
    "        result = []\n",
    "\n",
    "        # for every column\n",
    "        for c in list(all_features):\n",
    "            if c != 'target':\n",
    "\n",
    "                # add column\n",
    "                existing.append(c)\n",
    "\n",
    "                # new dataset\n",
    "                df = df_combine[existing]\n",
    "\n",
    "                # build model\n",
    "                test, train = split_test_train(df)\n",
    "                y_train = train['target'].round(0).astype(int)\n",
    "                y_test = test['target'].round(0).astype(int)\n",
    "                x_train = train.drop(['target'], axis=1)\n",
    "                x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "                # split train on train and validation\n",
    "                X_train_final, X_test_validation, y_train_final, y_test_validation = train_test_split(x_train, \n",
    "                                                                                                      y_train, \n",
    "                                                                                                      stratify=y_train,\n",
    "                                                                                                      test_size=0.25)\n",
    "        \n",
    "                # create datasets\n",
    "                d_train = lg.Dataset(X_train_final, label=y_train_final)\n",
    "                d_test = lg.Dataset(X_test_validation, label=y_test_validation, reference=d_train)\n",
    "\n",
    "                params={}\n",
    "                params['boosting_type']='dart' \n",
    "                params['objective']='binary' \n",
    "                params['metric']='auc' \n",
    "                params['verbosity'] = 0\n",
    "\n",
    "                # train the model \n",
    "                regressor=lg.train(params = params, \n",
    "                              train_set = d_train,\n",
    "                              valid_sets = d_test, \n",
    "                              num_boost_round = self.num_of_boosting_rounds)\n",
    "\n",
    "                # calculate metric\n",
    "                predicted = regressor.predict(x_train)\n",
    "\n",
    "                try:\n",
    "                    roc_auc = Metrics.roc_auc(y_train, predicted)\n",
    "                except ValueError:\n",
    "                    roc_auc = -1\n",
    "\n",
    "                # save result\n",
    "                diff = roc_auc - prev_roc_auc\n",
    "                item = (len(existing), existing, roc_auc, diff)\n",
    "                result.append(item)\n",
    "                if self.do_log == True: print(f'======== {len(existing)} -> AUC: {roc_auc} -> DIFF: {diff}' )\n",
    "\n",
    "                if (roc_auc > self.base_line_auc):\n",
    "                    if self.do_log == True: print(f'======== {roc_auc} increased base line threshold with {len(existing) -2} features {existing}')\n",
    "                    break\n",
    "\n",
    "                # handle negative impact\n",
    "                if (diff < -0.01):\n",
    "                    if self.do_log == True: print(f'======== Feature {c} gives negative impact of {diff}. Removing it.')\n",
    "                    existing.remove(c)\n",
    "                else:\n",
    "                    prev_roc_auc = roc_auc\n",
    "        \n",
    "        # save result to file\n",
    "        pickle.dump(result, open(self.results_file, 'wb'))\n",
    "        pickle.dump(regressor, open(self.model_file, 'wb'))\n",
    "        pickle.dump(df, open(self.df_file, 'wb'))\n",
    "        self.regressor = regressor\n",
    "        \n",
    "        # make prediction\n",
    "        self.train_predicted = self.regressor.predict(x_train)\n",
    "        self.test_predicted = self.regressor.predict(x_test)\n",
    "        \n",
    "        # data for metrics calc\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test \n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        pass\n",
    "    \n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return self.result[-1][1][2:]\n",
    "    \n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        return self.result[-1][1][2:]\n",
    "    \n",
    "    def get_test_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_test, self.test_predicted)\n",
    "\n",
    "    def get_train_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_train, self.train_predicted)\n",
    "\n",
    "    def get_test_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_train, self.train_predicted)\n",
    "    \n",
    "    def get_test_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_train, self.train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710153e5",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effae5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformPipe:\n",
    "    \n",
    "    def __init__(self, funcs:List[Callable], **kwargs):\n",
    "        self.funcs = funcs\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def transform(self, df:DataFrame) -> DataFrame:\n",
    "        for f in self.funcs:\n",
    "            df = f(df, **self.kwargs)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76692925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs -> col_names_to_impute, verbose, impute_method\n",
    "def impute_numeric_cols(df:DataFrame, **kwargs) -> DataFrame:\n",
    "    \n",
    "    impute_method = kwargs['impute_method']\n",
    "    \n",
    "    col_names = kwargs['col_names_to_impute']\n",
    "    \n",
    "    # all possible columns\n",
    "    initial_cols = list(df.columns)\n",
    "    \n",
    "    for col_name in col_names:\n",
    "\n",
    "        non_empty_numeric =  (set(get_number_cols(df)) - set(get_empty_cols(df))) | {col_name}        \n",
    "        cols_names_to_drop = list((set(df.columns) - non_empty_numeric))\n",
    "        \n",
    "        if (kwargs['verbose']==True): print('Deleting: {0}. Imputing: {1}'.format(cols_names_to_drop, col_name))\n",
    "        \n",
    "        # save temp\n",
    "        temp = df[cols_names_to_drop]\n",
    "\n",
    "        # clear dataset \n",
    "        df = df.drop(columns=cols_names_to_drop)\n",
    "        \n",
    "        known = df.loc[ df[col_name].notnull() ]        \n",
    "        unknown = df.loc[ df[col_name].isnull() ]\n",
    "        \n",
    "        # nothing to predict\n",
    "        if (len(unknown) == 0): \n",
    "            if (kwargs['verbose']==True): print('Nothing to predict - continue')\n",
    "            continue\n",
    "        \n",
    "        column_index = list(df.columns).index(col_name)\n",
    "\n",
    "        all_indices = [i for i in range(unknown.shape[1])]\n",
    "        diff = list(set(all_indices) - {column_index})\n",
    "\n",
    "        y = known.values[:, column_index]\n",
    "        X = known.values[:, diff]\n",
    "\n",
    "        # select regressor\n",
    "        if impute_method == 'randomforest':\n",
    "            regressor = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        if impute_method == 'knn':\n",
    "            regressor = KNeighborsRegressor()\n",
    "            \n",
    "        regressor.fit(X, y)\n",
    "        predicted = regressor.predict(unknown.values[:, diff])\n",
    "        \n",
    "        if (kwargs['verbose']==True): print('{0} was predicted. Len: {1}'.format(col_name, len(predicted)))\n",
    "        \n",
    "        # fill missings\n",
    "        df.loc[ (df[col_name].isnull()), col_name ] = predicted\n",
    "        \n",
    "        # restore dataset\n",
    "        df[cols_names_to_drop] = temp\n",
    "    \n",
    "    # reorder columns back\n",
    "    df = df.reindex(columns = initial_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91279004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_skew(df:DataFrame, **kwargs) -> DataFrame:\n",
    "    \n",
    "    excluded_features = kwargs['excluded_features']\n",
    "    \n",
    "    skew_df = pd.DataFrame(get_number_cols(df), columns=['Feature'])\n",
    "    skew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(df[feature]))\n",
    "    skew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\n",
    "    skew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\n",
    "\n",
    "    for column in skew_df.query(\"Skewed == True\")['Feature'].values:\n",
    "        if (column not in excluded_features):\n",
    "            df[column] = np.log1p(df[column])\n",
    "        else:\n",
    "            print(f'Skipping column: {column}')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b6fea",
   "metadata": {},
   "source": [
    "### Threshold selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd7846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdHelper:\n",
    "    \n",
    "    def apply_binary_threshold(threshold:float, y_predicted):\n",
    "        result = []\n",
    "\n",
    "        for y in y_predicted:\n",
    "            if (y >= threshold):\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b2c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_threshold(self, y_true, y_predicted) -> float:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cfb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ThresholdSelector(ThresholdSelector):\n",
    "    \n",
    "    def get_f1(self, y_real, y_predicted) -> float:\n",
    "        precision, recall, thresholds = precision_recall_curve(y_real, y_predicted)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        ix = argmax(fscore)\n",
    "        f_score = fscore[ix]\n",
    "        return f_score\n",
    "    \n",
    "    def get_threshold(self, y_real, y_predicted) -> float:\n",
    "        precision, recall, thresholds = precision_recall_curve(y_real, y_predicted)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        ix = argmax(fscore)\n",
    "        best_threshold = thresholds[ix]\n",
    "        return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd68b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceBasedThresholdSelector(ThresholdSelector):\n",
    "    \n",
    "    def __init__(self, add_new_abon_price:float, retain_abon_price:float, plot:bool):\n",
    "        self.plot = plot\n",
    "        self.add_new_abon_price = add_new_abon_price\n",
    "        self.retain_abon_price = retain_abon_price\n",
    "        self.plot \n",
    "    \n",
    "    def price_for_threshold(self, threshold) -> float:\n",
    "        ix = self.X.index(threshold)\n",
    "        return self.Y[ix]\n",
    "    \n",
    "    def get_threshold(self, y_real, y_predicted) -> float:\n",
    "        \n",
    "        X = []\n",
    "        TNs = []\n",
    "        FNs = []\n",
    "        TPs = []\n",
    "        FPs = []\n",
    "        Y = []\n",
    "        \n",
    "        for thr in np.arange (0, 1, 0.01):\n",
    "            test_predicted_final = ThresholdHelper.apply_binary_threshold(thr, y_predicted)\n",
    "            conf_m = Metrics.confusion_matrix(y_real, test_predicted_final)\n",
    "\n",
    "            TN = conf_m[0][0] \n",
    "            FN = conf_m[0][1]\n",
    "            TP = conf_m[1][1]\n",
    "            FP = conf_m[1][0]\n",
    "\n",
    "            final_price = TP*self.retain_abon_price + FP*self.add_new_abon_price + FN*self.add_new_abon_price\n",
    "\n",
    "            X.append(thr)            \n",
    "            TNs.append(TN)\n",
    "            FNs.append(FN)\n",
    "            TPs.append(TP)\n",
    "            FPs.append(FP)\n",
    "            Y.append(final_price)\n",
    "            \n",
    "        best_threshold = X[Y.index(min(Y))]        \n",
    "        f1_selector = F1ThresholdSelector()\n",
    "        f1_threshold = f1_selector.get_threshold(y_real, y_predicted)\n",
    "        \n",
    "        if self.plot == True:\n",
    "                    \n",
    "            threshold_price_X = [best_threshold, best_threshold]\n",
    "            threshold_price_Y = [0, max([*TPs, *FPs, *FNs])]\n",
    "            \n",
    "            threshold_f1_X = [f1_threshold, f1_threshold]\n",
    "            threshold_f1_Y = [0, max([*TPs, *FPs, *FNs])]\n",
    "            \n",
    "            # price\n",
    "            Metrics.XY_plot([X, threshold_f1_X, threshold_price_X],\n",
    "                            [Y, [0, max(Y)], [0, max(Y)]], \n",
    "                            ['Price', 'F1 threshold', 'Price threshold'],\n",
    "                            'threshold', 'price')\n",
    "\n",
    "            # metrics\n",
    "            Metrics.XY_plot([X, X, X, threshold_f1_X, threshold_price_X ],\n",
    "                            [TPs, FPs, FNs, threshold_f1_Y, threshold_price_Y ],\n",
    "                            ['TPs', 'FPs', 'FNs', 'F1 threshold', 'Price threshold'], \n",
    "                            'threshold', 'metrics')\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.best_threshold = best_threshold\n",
    "        \n",
    "        return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c2cb6",
   "metadata": {},
   "source": [
    "### Full experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "716c4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, \n",
    "                data:DataFrame,\n",
    "                selector:FeatureSelector, \n",
    "                transformer:TransformPipe):\n",
    "        self.data = data\n",
    "        self.selector = selector\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def run(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688eb39d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d3baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_num, train_dpi, test_df, test_num, test_dpi = load_churn_reduced()\n",
    "df_combine, df_combine_num, df_combine_dpi = combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f98699",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5393bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43\n",
    "# https://www.kaggle.com/code/gomes555/tps-jun2021-feature-selection-lightgbm-tuner\n",
    "\n",
    "pearsonSelector = CorrelationSelector(df_combine, 'pearson', -1)\n",
    "spearmanSelector = CorrelationSelector(df_combine, 'spearman', -1)\n",
    "normal = LGBMSelector(df_combine, -1)\n",
    "enhanced = EnhancedLGBMSelector(df_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4a0d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.954024923900115, 0.8980686013446386)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.get_train_ROCAUC(), normal.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b7a538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9310064543985634, 0.8964773842119418)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced.get_train_ROCAUC(), enhanced.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e748f917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 27)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_non_corr = pearsonSelector.get_important_noncorrelated_features_tuples(0.95, 0.2)\n",
    "spearman_non_corr = spearmanSelector.get_important_noncorrelated_features_tuples(0.95, 0.2)\n",
    "len(pearson_non_corr), len(spearman_non_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfada5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8916336158130531, 0.8543018544980374)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [*[f[0] for f in pearson_non_corr], *['target', 'ind']]\n",
    "df_temp = df_combine[features]\n",
    "pearson_test_selector = EnhancedLGBMSelector(df_temp, '___pearson_enhanced.plc')\n",
    "pearson_test_selector.get_train_ROCAUC(), pearson_test_selector.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c607f80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.878686640995647, 0.8449606682497676)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [*[f[0] for f in spearman_non_corr], *['target', 'ind']]\n",
    "df_temp = df_combine[features]\n",
    "spearman_test_selector = EnhancedLGBMSelector(df_temp, '___spearman_enhanced.plc')\n",
    "spearman_test_selector.get_train_ROCAUC(), spearman_test_selector.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0ed60d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iterative_150 \u001b[38;5;241m=\u001b[39m \u001b[43mIterativeLGBMSelector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.89\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m iterativeFeatures \u001b[38;5;241m=\u001b[39m iterative_150\u001b[38;5;241m.\u001b[39mget_important_features()\n\u001b[0;32m      3\u001b[0m results_150 \u001b[38;5;241m=\u001b[39m iterative_150\u001b[38;5;241m.\u001b[39mget_training_results()\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mIterativeLGBMSelector.__init__\u001b[1;34m(self, data, selector, base_line_auc, num_of_boosting_rounds, do_log)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   3536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3537\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 3538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3539\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3540\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    846\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 848\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:908\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[1;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    907\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 908\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_API_IS_ROW_MAJOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterative_150 = IterativeLGBMSelector(df_combine, normal, 0.89, 150, True)\n",
    "iterativeFeatures = iterative_150.get_important_features()\n",
    "results_150 = iterative_150.get_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_150.get_train_ROCAUC(), iterative_150.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [r[0] for r in results_150]\n",
    "Y = [r[2] for r in results_150]\n",
    "Metrics.XY_plot([X], [Y], ['Num of features VS AUC'], 'Num of features', 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2baa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [f'Default {df_combine.shape[1]}',\n",
    "     f'Cross-val {df_combine.shape[1]}',\n",
    "     f'Pearson non-cor {len(pearson_non_corr)}',\n",
    "     f'Spearman non-cor {len(spearman_non_corr)}',\n",
    "     f'Iterative {len(iterativeFeatures)}' ]\n",
    "\n",
    "Y = [normal.get_test_ROCAUC(),\n",
    "     enhanced.get_test_ROCAUC(),\n",
    "     pearson_test_selector.get_test_ROCAUC(),\n",
    "     spearman_test_selector.get_test_ROCAUC(),\n",
    "     iterative_150.get_test_ROCAUC() ]\n",
    "\n",
    "Metrics.BAR_plot(X, Y, 'Models AUC comparison', 'Model name', 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e692ee",
   "metadata": {},
   "source": [
    "## Filling Missing Data\n",
    "### Selecting iterative model, because it showed best AUC with less num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures = [*[f for f in iterativeFeatures], *['target', 'ind']]\n",
    "df_final = df_combine[finalFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_path = '___final_dataset_knn.pcl'\n",
    "\n",
    "if os.path.exists(imputed_path) == True:\n",
    "    df_final = pickle.load(open(imputed_path, 'rb'))\n",
    "else:\n",
    "    # kwargs -> col_names_to_impute, verbose, impute_method\n",
    "    df_final = impute_numeric_cols(df_final, col_names_to_impute = get_empty_cols(df_final), verbose = True, impute_method = 'knn')\n",
    "    pickle.dump(df_final, open(imputed_path, 'wb'))\n",
    "    \n",
    "df_final = remove_skew(df_final, excluded_features = ['target'])\n",
    "kkn_no_skew = EnhancedLGBMSelector(df_final, 'kkn_no_skew.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_path = '___final_dataset_randomforest.pcl'\n",
    "\n",
    "if os.path.exists(imputed_path) == True:\n",
    "    df_final = pickle.load(open(imputed_path, 'rb'))\n",
    "else:\n",
    "    # kwargs -> col_names_to_impute, verbose, impute_method\n",
    "    df_final = impute_numeric_cols(df_final, col_names_to_impute = get_empty_cols(df_final), verbose = True, impute_method = 'randomforest')\n",
    "    pickle.dump(df_final, open(imputed_path, 'wb'))\n",
    "    \n",
    "df_final = remove_skew(df_final, excluded_features = ['target'])\n",
    "randomforest_no_skew = EnhancedLGBMSelector(df_final, 'randomforest_no_skew.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ 'Iterative' + ' ' + str(len(iterativeFeatures)),\n",
    "      'Knn No Skew',\n",
    "      'Random Forest No Skew' ]\n",
    "\n",
    "Y = [ results[-1][2], kkn_no_skew.get_ROCAUC(), randomforest_no_skew.get_ROCAUC() ]\n",
    "  \n",
    "Metrics.BAR_plot(X, Y, 'Models AUC comparison', 'Model name', 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59513712",
   "metadata": {},
   "source": [
    "### Threshold + Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection\n",
    "best_model = normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/96690/how-to-choose-the-right-threshold-for-binary-classification\n",
    "# https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/\n",
    "# https://ploomber.io/blog/threshold/\n",
    "\n",
    "Metrics.plot_auc([best_model.y_train, best_model.y_test], [best_model.train_predicted,  best_model.test_predicted], ['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65930111",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_selector = F1ThresholdSelector()\n",
    "f1_threshold = f1_selector.get_threshold(best_model.y_train, best_model.train_predicted)\n",
    "print(f'F1 based threshold: {f1_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_based_selector = PriceBasedThresholdSelector(1000, 50, True)\n",
    "price_based_thr = price_based_selector.get_threshold(best_model.y_train, best_model.train_predicted)\n",
    "print(f'Price based threshold: {price_based_thr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_price = price_based_selector.price_for_threshold(round(f1_threshold, 2))\n",
    "price_based = price_based_selector.price_for_threshold(round(price_based_thr, 2))\n",
    "print(f'f1 price: {f1_price}. Price based: {price_based}. Diff: {f1_price - price_based}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86210570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_f1_threshold = ThresholdHelper.apply_binary_threshold(f1_threshold, best_model.train_predicted)\n",
    "ConfusionMatrixDisplay.from_predictions(best_model.y_train, with_f1_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fe7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_price_threshold = ThresholdHelper.apply_binary_threshold(price_based_thr, best_model.train_predicted)\n",
    "ConfusionMatrixDisplay.from_predictions(best_model.y_train, with_price_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab493d",
   "metadata": {},
   "source": [
    "### Check if dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned = len(df_combine[df_combine['target'] == 1])\n",
    "not_churned = len(df_combine[df_combine['target'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [churned, not_churned]\n",
    "x = ['Churned', 'Not churned']\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd1142",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "#### Check inbound calls from non-vodaphone number in min\n",
    "#### Check outbound calls to non-vodaphone number in min\n",
    "#### SMS from non-vodaphone number count\n",
    "#### SMS to non-vodaphone number count\n",
    "#### Calls to pawnshop, micro credit organizations in mins\n",
    "#### Calls from pawnshop, micro credit organizations in mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8d68b",
   "metadata": {},
   "source": [
    "## Telephones features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b81fec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelephoneHelper:\n",
    "    \n",
    "    def is_short_number(number:str) -> bool:\n",
    "        if (number.isdigit() and len(number) <= 4):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_life(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['063', '093', '073' ])):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_kyivstar(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['067', '097', '068', '098', '098'])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['050', '095', '099', '066' ])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_non_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and not TelephoneHelper.is_vodafone(number)):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_string_number(number:str) -> bool:\n",
    "        if (number.isdigit() == False):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "de8eecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_num, train_dpi, test_df, test_num, test_dpi = load_churn_reduced()\n",
    "df_combine, df_combine_num, df_combine_dpi = combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f46ce76b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[230], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m churned \u001b[38;5;241m=\u001b[39m df_combine[df_combine[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m churned_with_nums \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchurned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_combine_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabon_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m churned_numbers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(churned_with_nums[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbnum\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m      5\u001b[0m non_churned \u001b[38;5;241m=\u001b[39m df_combine[df_combine[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    699\u001b[0m (\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m--> 703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1232\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_drop_labels_or_levels(left_drop)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_drop:\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_labels_or_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m left_keys, right_keys, join_names\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1920\u001b[0m, in \u001b[0;36mNDFrame._drop_labels_or_levels\u001b[1;34m(self, keys, axis)\u001b[0m\n\u001b[0;32m   1915\u001b[0m labels_to_drop \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_level_reference(k, axis\u001b[38;5;241m=\u001b[39maxis)]\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;66;03m# Perform copy upfront and then use inplace operations below.\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m \u001b[38;5;66;03m# This ensures that we always perform exactly one copy.\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m \u001b[38;5;66;03m# ``copy`` and/or ``inplace`` options could be added in the future.\u001b[39;00m\n\u001b[1;32m-> 1920\u001b[0m dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1923\u001b[0m     \u001b[38;5;66;03m# Handle dropping index levels\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m levels_to_drop:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6368\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6258\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6260\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6261\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6366\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6368\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:670\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    667\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 670\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1871\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m _consolidate_with_refs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2329\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2327\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2329\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2332\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2392\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2389\u001b[0m     new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2391\u001b[0m     bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n\u001b[1;32m-> 2392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mnew_block_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbp\u001b[49m\u001b[43m)\u001b[49m], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n\u001b[0;32m   2395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2161\u001b[0m, in \u001b[0;36mnew_block_2d\u001b[1;34m(values, placement)\u001b[0m\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m ObjectBlock\n\u001b[0;32m   2158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m-> 2161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_block_2d\u001b[39m(values: ArrayLike, placement: BlockPlacement):\n\u001b[0;32m   2162\u001b[0m     \u001b[38;5;66;03m# new_block specialized to case with\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m     \u001b[38;5;66;03m#  ndim=2\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[38;5;66;03m#  isinstance(placement, BlockPlacement)\u001b[39;00m\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;66;03m#  check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m     klass \u001b[38;5;241m=\u001b[39m get_block_type(values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2168\u001b[0m     values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "churned = df_combine[df_combine['target'] == 1]\n",
    "churned_with_nums = pd.merge(churned, df_combine_num, on='abon_id', how='left')\n",
    "churned_numbers = list(churned_with_nums['bnum'].unique())\n",
    "\n",
    "non_churned = df_combine[df_combine['target'] == 0]\n",
    "non_churned_with_nums = pd.merge(non_churned, df_combine_num, on='abon_id', how='left')\n",
    "non_churned_numbers = list(non_churned_with_nums['bnum'].unique())\n",
    "\n",
    "number_abon_had_communicated = (set(churned_numbers) - set(non_churned_numbers))\n",
    "df_number_abon_had_communicated = pd.DataFrame(number_abon_had_communicated, columns= ['bnum'])\n",
    "\n",
    "temp = churned_with_nums[churned_with_nums['bnum'].isin(list(number_abon_had_communicated))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65885d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_nums = [t for t in number_abon_had_communicated if TelephoneHelper.is_short_number(t) or TelephoneHelper.is_string_number(t)]\n",
    "print(f'Numbers churned abonents communicated with: {short_nums}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c007707",
   "metadata": {},
   "outputs": [],
   "source": [
    "__PAWNSHOPS__ = [b'lombard zf', b'money_point', b'whitecredi', b'vsezaimy', b'tvoyapozyk']\n",
    "\n",
    "call_dur_in_mask = (df_combine_num['call_dur_in'] > 0)\n",
    "call_dur_out_mask = (df_combine_num['call_dur_out'] > 0)\n",
    "sms_cnt_in_mask = (df_combine_num['cnt_sms_in'] > 0) \n",
    "sms_cnt_out_mask = (df_combine_num['cnt_sms_out'] > 0) \n",
    "non_vodafone_num_mask = (df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone))\n",
    "pawnshop_num_mask = (df_combine_num['bnum'].isin(__PAWNSHOPS__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f993d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_non_vodafone_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (call_dur_in_mask) & (non_vodafone_num_mask)]\n",
    "    \n",
    "    return sum(list(data['call_dur_in']))\n",
    "\n",
    "def get_outgoing_non_vodafone_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (call_dur_out_mask) & (non_vodafone_num_mask)]\n",
    "    \n",
    "    return sum(list(data['call_dur_out']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ae365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_non_vodafone_sms_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (sms_cnt_in_mask) & (non_vodafone_num_mask)]\n",
    "    \n",
    "    return sum(list(data['cnt_sms_in']))\n",
    "\n",
    "def get_outgoing_non_vodafone_sms_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (sms_cnt_out_mask) & (non_vodafone_num_mask)]\n",
    "    \n",
    "    return sum(list(data['cnt_sms_out']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_pawnshops_sms_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (sms_cnt_in_mask) & (pawnshop_num_mask)]\n",
    "    \n",
    "    return sum(list(data['cnt_sms_in']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc207ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine['incoming_call_non_vodafone'] = df_combine['abon_id'].apply(get_incoming_non_vodafone_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine['outgoing_call_non_vodafone'] = df_combine['abon_id'].apply(get_outgoing_non_vodafone_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f22500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine['incoming_sms_non_vodafone'] = df_combine['abon_id'].apply(get_incoming_non_vodafone_sms_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine['outgoing_sms_non_vodafone'] = df_combine['abon_id'].apply(get_outgoing_non_vodafone_sms_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87716bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine['incoming_call_pawnshop'] = df_combine['abon_id'].apply(get_incoming_pawnshops_sms_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5704dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_enhanced_telephone_path = '___save_enhanced_telephone_path.pkl'\n",
    "if os.path.exists(save_enhanced_bnum_path) == False:\n",
    "    pickle.dump(df_tmp, open(save_enhanced_bnum_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043836f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61e2d077",
   "metadata": {},
   "source": [
    "## Application Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5c737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75958788",
   "metadata": {},
   "source": [
    "## Groupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = train_num.groupby(['abon_id'])\n",
    "gr.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6992251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_matrix = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c437c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6700b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facebe50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad075db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c644d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9765e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d167d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e5976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbbe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84dcf0a",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
