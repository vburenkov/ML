{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a29a5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lg\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "import string \n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "from typing import List, TypeVar, Dict, Callable\n",
    "import abc\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, precision_score, average_precision_score, roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from numpy import argmax\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8636c9",
   "metadata": {},
   "source": [
    "0) use predictproba instead of predict +\n",
    "1) fix train set +\n",
    "2) calculate threshold (precision, recall, f1) +\n",
    "3) probability distribution\n",
    "4) Lecture 6.1, 6.0.1\n",
    "5) SHAP\n",
    "6) Exploratory analysis\n",
    "7) Feature engineering\n",
    "\n",
    "====\n",
    "\n",
    "15 - 400\n",
    "23 - 150\n",
    "50 - 0.895\n",
    "\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78db6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_СORRELATION_MATRIX_PATH_ = 'corr_matrix.pcl'\n",
    "_СHURN_PATH_ = 'churn_model.pcl'\n",
    "_CHURN_PATH_REDUCED_ = 'churn_model_reduced.pcl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3885d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077\n",
    "# https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "# https://www.statology.org/plot-roc-curve-python/\n",
    "# https://analyticsindiamag.com/complete-guide-to-lightgbm-boosting-algorithm-in-python/\n",
    "class Metrics:\n",
    "    \n",
    "    def roc_auc(y_true, predicted):\n",
    "        return roc_auc_score(y_true, predicted)\n",
    "    \n",
    "    def plot_auc(y_true:list, predicted:list, labels:list):\n",
    "        \n",
    "        for i in range(0, len(y_true)):\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_true[i], predicted[i])\n",
    "            auc = metrics.roc_auc_score(y_true[i], predicted[i])\n",
    "            plt.plot(fpr,tpr, label=f\"{labels[i]} AUC=\"+str(auc))\n",
    "\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "    \n",
    "    def XY_plot(Xs:list, Ys:list, legends:list, x_label:str, y_label:str):\n",
    "        \n",
    "        for i in range(0, len(Xs)):      \n",
    "            plt.plot(Xs[i], Ys[i], label=legends[i])\n",
    "        \n",
    "        plt.ylabel(y_label)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "    \n",
    "    def BAR_plot(X, Y, title:str, x_label:str, y_label:str):\n",
    "        fig = plt.figure(figsize = (10, 5))\n",
    "        plt.bar(X, Y, color ='maroon', width = 0.4)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def classification_report(y_true, predicted):\n",
    "        return classification_report(y_true, predicted)\n",
    "    \n",
    "    def confusion_matrix(y_true, predicted):\n",
    "        return confusion_matrix(y_true, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77db8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_string(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    result_str = ''.join(random.choice(letters) for i in range(length))\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1480700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_float_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=float).columns.tolist()\n",
    "\n",
    "def get_int_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(include=int).columns.tolist()\n",
    "\n",
    "def get_number_cols(df:DataFrame) -> List[str]:\n",
    "    return df.select_dtypes(np.number).columns.tolist()\n",
    "\n",
    "def get_obj_cols(df:DataFrame) -> List[str]:\n",
    "    return list(df.select_dtypes(include=object).columns)\n",
    "    \n",
    "def get_empty_cols(df:DataFrame):\n",
    "    return list(df.isnull().sum()[lambda x: x > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1207571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True) -> pd.DataFrame:\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0ffcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(df:DataFrame):\n",
    "    test, train = df[df['ind'].eq('test')], df[df['ind'].eq('train')]\n",
    "    test = test.drop(['ind'], axis=1)\n",
    "    train = train.drop(['ind'], axis=1)\n",
    "    return test, train\n",
    "    \n",
    "def combine_test_train(test:DataFrame, train:DataFrame):\n",
    "    combine = pd.concat([test.assign(ind='test'), train.assign(ind='train')])\n",
    "    target = train['target']\n",
    "    test_ids = test['Id']\n",
    "    return combine, target, test_ids\n",
    "\n",
    "def combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi):\n",
    "    df_combine = pd.concat([train_df.assign(ind='train'), test_df.assign(ind='test')])\n",
    "    df_combine_num = pd.concat([train_num.assign(ind='train'), test_num.assign(ind='test')])\n",
    "    df_combine_dpi = pd.concat([train_dpi.assign(ind='train'), test_dpi.assign(ind='test')])\n",
    "    return df_combine, df_combine_num, df_combine_dpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65fd4a",
   "metadata": {},
   "source": [
    "## Data size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f25481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_churn_data():\n",
    "\n",
    "    if os.path.exists(_CHURN_PATH_REDUCED_) == False:\n",
    "        if os.path.exists(_СHURN_PATH_) == True: \n",
    "    \n",
    "            with open(_СHURN_PATH_, 'rb') as file:\n",
    "                deserialized_object = pickle.load(file)\n",
    "\n",
    "            deserialized_object = list(deserialized_object)\n",
    "        \n",
    "            # reduce size\n",
    "            train_df= reduce_mem_usage(deserialized_object[1][1])\n",
    "            train_num_reduced = reduce_mem_usage(deserialized_object[1][2])\n",
    "            train_dpi_reduced = reduce_mem_usage(deserialized_object[1][3])\n",
    "\n",
    "            test_df = reduce_mem_usage(deserialized_object[2][1])\n",
    "            test_num_reduced = reduce_mem_usage(deserialized_object[2][2])\n",
    "            test_dpi_reduced = reduce_mem_usage(deserialized_object[2][3])\n",
    "            \n",
    "            # dump data back\n",
    "            deserialized_object = tuple([[train_df, train_num_reduced, train_dpi_reduced], [test_df, test_num_reduced, test_dpi_reduced]])\n",
    "            pickle.dump(deserialized_object, open(_CHURN_PATH_REDUCED_, 'wb'))\n",
    "    else:\n",
    "        print(f'{_CHURN_PATH_REDUCED_} already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6e18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_data():\n",
    "\n",
    "    with open(_СHURN_PATH_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[1][1]\n",
    "    train_num = deserialized_object[1][2]\n",
    "    train_dpi = deserialized_object[1][3]\n",
    "\n",
    "    test_df = deserialized_object[2][1]\n",
    "    test_num = deserialized_object[2][2]\n",
    "    test_dpi = deserialized_object[2][3]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d952ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_churn_reduced():\n",
    "\n",
    "    with open(_CHURN_PATH_REDUCED_, 'rb') as file:\n",
    "        deserialized_object = pickle.load(file)\n",
    "\n",
    "    train_df = deserialized_object[0][0]\n",
    "    train_num = deserialized_object[0][1]\n",
    "    train_dpi = deserialized_object[0][2]\n",
    "\n",
    "    test_df = deserialized_object[1][0]\n",
    "    test_num = deserialized_object[1][1]\n",
    "    test_dpi = deserialized_object[1][2]\n",
    "\n",
    "    return train_df, train_num, train_dpi, test_df, test_num, test_dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f00265aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelephoneHelper:\n",
    "    \n",
    "    def is_short_number(number:str) -> bool:\n",
    "        if (number.isdigit() and len(number) <= 4):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_life(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['063', '093', '073' ])):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_kyivstar(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['067', '097', '068', '098', '098'])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and (number[2:5] in ['050', '095', '099', '066' ])):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_non_vodafone(number:str) -> bool:\n",
    "        if (len(number) == 12 and not TelephoneHelper.is_vodafone(number)):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_string_number(number:str) -> bool:\n",
    "        if (number.isdigit() == False):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca85d64",
   "metadata": {},
   "source": [
    "### Feature selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7024508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureHelper:\n",
    "    \n",
    "    def get_feature_correlation_df(corr_m, remove_duplicates=True, remove_self_correlations=True):\n",
    "    \n",
    "        corr_matrix_abs = corr_m.abs()\n",
    "        corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
    "        sorted_correlated_features = corr_matrix_abs_us \\\n",
    "            .sort_values(kind=\"quicksort\", ascending=False) \\\n",
    "            .reset_index()\n",
    "\n",
    "        # Remove comparisons of the same feature\n",
    "        if remove_self_correlations:\n",
    "            sorted_correlated_features = sorted_correlated_features[\n",
    "                (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
    "            ]\n",
    "\n",
    "        # Remove duplicates\n",
    "        if remove_duplicates:\n",
    "            sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
    "\n",
    "        # Create meaningful names for the columns\n",
    "        sorted_correlated_features.columns = ['f1', 'f2', 'corr']\n",
    "\n",
    "        return sorted_correlated_features\n",
    "    \n",
    "    def get_correlation_matrix(df:DataFrame, method:str, save_path:str):\n",
    "        if os.path.exists(save_path) == False:\n",
    "            corr_matrix = df.corr(method = method, numeric_only = True)\n",
    "            pickle.dump(corr_matrix, open(save_path, 'wb'))\n",
    "        else:\n",
    "            corr_matrix = pickle.load(open(save_path, 'rb'))\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def remove_aggr_function(str_to_check:str) -> str:\n",
    "        parts = str_to_check.split('_')\n",
    "        \n",
    "        if (len(parts) > 2):\n",
    "            index_to_remove = len(parts) - 2\n",
    "            \n",
    "            # remove aggregation function\n",
    "            if (parts[index_to_remove] in ['min', 'std', 'max', 'mea', 'td']):\n",
    "                parts.remove(parts[index_to_remove])\n",
    "                \n",
    "            result = '_'.join(parts)\n",
    "            return result\n",
    "        else:\n",
    "            return str_to_check    \n",
    "\n",
    "        \n",
    "    def get_heatmap_matrix(corr_matrix:DataFrame):\n",
    "        heatmap_matrix = pd.DataFrame(corr_matrix['target'].abs())\n",
    "        heatmap_matrix = heatmap_matrix.sort_values(by='target', ascending=False)\n",
    "        heatmap_matrix = heatmap_matrix.drop(index=['target'])           \n",
    "        return heatmap_matrix\n",
    "    \n",
    "    # index - column name\n",
    "    # target - value\n",
    "    def plot_heatmap(heatmap_matrix:DataFrame):\n",
    "        plt.figure(figsize=(40, 120))\n",
    "        heatmap = sns.heatmap(heatmap_matrix, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "        heatmap.set_title('Features Correlating with Churn Rate', fontdict={'fontsize':18}, pad=16);\n",
    "        return heatmap_matrix\n",
    "    \n",
    "    def get_important_features(heatmap_matrix:DataFrame, use_groupping = False, num_of_features:int = -1):\n",
    "        df_features = heatmap_matrix.reset_index()\n",
    "        df_features = df_features.rename(columns = {'index':'feature'})\n",
    "        \n",
    "        # apply aggregation function for further groupping\n",
    "        df_features['feature_group'] = df_features['feature'].apply(FeatureHelper.remove_aggr_function)\n",
    "        df_features = df_features[['feature', 'feature_group', 'target']]\n",
    "        sorted_features = df_features.sort_values(by=['feature_group', 'target'], ascending = [False, False])\n",
    "        \n",
    "        # take first item from the group\n",
    "        if (use_groupping == True):\n",
    "            important_features = sorted_features.groupby('feature_group').first()\n",
    "        else:\n",
    "            important_features = sorted_features\n",
    "\n",
    "        # order by target\n",
    "        important_features = important_features.sort_values(by='target', ascending=False)\n",
    "           \n",
    "        # take N first rows\n",
    "        if (num_of_features != -1):\n",
    "            important_features = important_features.head(num_of_features)\n",
    "        \n",
    "        # optimize for heatmap\n",
    "        important_features = important_features.reset_index()\n",
    "        important_features = important_features[['feature', 'target']]\n",
    "        important_features.index = important_features['feature']\n",
    "        important_features.index.name = None\n",
    "        important_features = important_features[['target']]\n",
    "        important_features = important_features[important_features['target'] > 0]\n",
    "        \n",
    "        return important_features\n",
    "    \n",
    "    def get_important_features_tuples(heatmap_matrix:DataFrame, num_of_features:int = -1):\n",
    "        important_features = FeatureHelper.get_important_features(heatmap_matrix, num_of_features)\n",
    "        \n",
    "        if (num_of_features == -1):\n",
    "            num_of_features = len(important_features)\n",
    "        \n",
    "        important_features_tuples = list(zip(important_features.index, \n",
    "                                             important_features.target, \n",
    "                                             list(range(0, num_of_features))))\n",
    "        \n",
    "        return important_features_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b170f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    # index - feature name, target\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    # 1 - feature name, 2 - target, 3 - sorted number\n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe1177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationSelector(FeatureSelector):\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, corr_method:str, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.corr_method = corr_method\n",
    "        self.file_prefix = corr_method\n",
    "        self.num_of_features = num_of_features\n",
    "        self.get_heatmap()\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        self.save_path = f'{self.file_prefix}_{_СORRELATION_MATRIX_PATH_}'\n",
    "        self.corr_m = FeatureHelper.get_correlation_matrix(self.data, self.corr_method, self.save_path)\n",
    "        self.heatmap_m = FeatureHelper.get_heatmap_matrix(self.corr_m)\n",
    "        return self.heatmap_m \n",
    "    \n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "    \n",
    "    def get_non_correlated_features(self, barrier_coef:float, do_log:bool) -> List[str]:\n",
    "        \n",
    "        important_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        # f1, f2, corr\n",
    "        features_corr = FeatureHelper.get_feature_correlation_df(self.corr_m)\n",
    "        \n",
    "        already_processed = set()\n",
    "        all_features = [t[0] for t in important_tuples]\n",
    "\n",
    "        for f in all_features:\n",
    "            # get correlated features\n",
    "            correlated = list(features_corr[(features_corr['f1']==f) & (features_corr['corr'] > barrier_coef)]['f2'])\n",
    "\n",
    "            # if highly correlated features exist\n",
    "            if (len(correlated)>0):\n",
    "\n",
    "                for to_remove in correlated:\n",
    "                    if (to_remove not in already_processed):\n",
    "                        if (to_remove in all_features):\n",
    "                            all_features.remove(to_remove)\n",
    "                            if do_log: print(f'Removing: {to_remove} for {f}')            \n",
    "\n",
    "        # remember initial feature\n",
    "        already_processed.add(f)\n",
    "        \n",
    "        # return non-correlated features\n",
    "        return all_features\n",
    "    \n",
    "    def get_important_noncorrelated_features_tuples(self, mutual_correlation:float, feature_importance:float):\n",
    "        \n",
    "        final = []\n",
    "        important = self.get_important_features_tuples()\n",
    "        non_correlated = self.get_non_correlated_features(mutual_correlation, False)\n",
    "        \n",
    "        for i in important:\n",
    "            if (i[0] in non_correlated and i[1] > feature_importance):\n",
    "                final.append(i)\n",
    "\n",
    "        return final\n",
    "                        \n",
    "    def get_important_features_tuples(self) ->  List[tuple[str, float, int]]:\n",
    "        return FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return type(self).__name__ + '_' + self.corr_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bd308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nitin9809.medium.com/lightgbm-binary-classification-multi-class-classification-regression-using-python-4f22032b36a2\n",
    "# https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n",
    "class LGBMSelector(FeatureSelector):\n",
    "    \n",
    "    model_file = '___LGBMSelector_simple.pcl'\n",
    "    heatmap_calculated = False\n",
    "    \n",
    "    def __init__(self, data:pd.DataFrame, num_of_features:int):\n",
    "        self.data = data\n",
    "        self.num_of_features = num_of_features\n",
    "        self.train_model()\n",
    "    \n",
    "    def train_model(self):\n",
    "        test, train = split_test_train(self.data)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        self.x_train = train.drop(['target'], axis=1)\n",
    "        self.x_test = test.drop(['target'], axis=1)\n",
    "        \n",
    "        # regressor\n",
    "        if os.path.exists(self.model_file) == True: \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "        else:         \n",
    "            self.regressor = lg.LGBMClassifier()\n",
    "            self.regressor.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        # predict\n",
    "        self.test_predicted = self.regressor.predict_proba(self.x_test)[:,1]\n",
    "        self.train_predicted = self.regressor.predict_proba(self.x_train)[:,1]\n",
    "        \n",
    "        #save model\n",
    "        pickle.dump(self.regressor, open(self.model_file, 'wb'))\n",
    "    \n",
    "    def get_feature_importance_raw(self):\n",
    "        return self.regressor.feature_importances_\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        df_feature_importance = pd.DataFrame(list(zip(list(self.get_feature_importance_raw()), list(self.data.columns))))\n",
    "        df_feature_importance = df_feature_importance.set_axis(['target', 'feature'], axis=1)\n",
    "        df_feature_importance = df_feature_importance.sort_values(by=['target'], ascending=False)\n",
    "        df_feature_importance = df_feature_importance.set_index('feature')\n",
    "        df_feature_importance.index.name = None\n",
    "        self.heatmap_m = df_feature_importance\n",
    "        self.heatmap_calculated = True\n",
    "        return self.heatmap_m\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        FeatureHelper.plot_heatmap(self.get_important_features())\n",
    "\n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        if (self.heatmap_calculated == False):\n",
    "            self.get_heatmap()\n",
    "            \n",
    "        return FeatureHelper.get_important_features(self.heatmap_m, self.num_of_features)\n",
    "    \n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        if (self.heatmap_calculated == False):\n",
    "            self.get_heatmap()\n",
    "            \n",
    "        features_tuples = FeatureHelper.get_important_features_tuples(self.heatmap_m, self.num_of_features)\n",
    "        return features_tuples\n",
    "        \n",
    "    def get_test_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_test, self.test_predicted)\n",
    "\n",
    "    def get_train_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_train, self.train_predicted)\n",
    "\n",
    "    def get_test_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_train, self.train_predicted)\n",
    "    \n",
    "    def get_test_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_train, self.train_predicted)\n",
    "\n",
    "    def __str__(self):\n",
    "        return type(self).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583ceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "class EnhancedLGBMSelector(LGBMSelector):\n",
    "    \n",
    "    def __init__(self, data, model_file_name:str = '___EnhancedLGBMSelector_model_800_features.pcl'):\n",
    "        self.model_file = model_file_name\n",
    "        super().__init__(data, -1)\n",
    "    \n",
    "    def get_feature_importance_raw(self):\n",
    "        return self.regressor.feature_importance()\n",
    "    \n",
    "    def train_model(self):        \n",
    "        \n",
    "        test, train = split_test_train(self.data)\n",
    "\n",
    "        self.y_train = train['target'].round(0).astype(int)\n",
    "        self.y_test = test['target'].round(0).astype(int)\n",
    "\n",
    "        self.x_train = train.drop(['target'], axis=1)\n",
    "        self.x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "        X_train_final, X_test_validation, y_train_final, y_test_validation = train_test_split(self.x_train, \n",
    "                                                                                              self.y_train, \n",
    "                                                                                              stratify=self.y_train,\n",
    "                                                                                              test_size=0.25)\n",
    "        \n",
    "        # Specifying datasets\n",
    "        d_train = lg.Dataset(X_train_final, label=y_train_final)\n",
    "        d_test = lg.Dataset(X_test_validation, label=y_test_validation, reference=d_train)\n",
    "\n",
    "        # load model from disk\n",
    "        if os.path.exists(self.model_file) == True: \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "        # Train model\n",
    "        else:\n",
    "            params={}\n",
    "            params['boosting_type']='dart' \n",
    "            params['objective']='binary' \n",
    "            params['metric']='auc' \n",
    "            params['verbosity'] = 2\n",
    "            params['is_unbalance'] = True\n",
    "\n",
    "            #train the model \n",
    "            self.regressor=lg.train(params = params, \n",
    "                                    train_set = d_train,\n",
    "                                    valid_sets = d_test, \n",
    "                                    num_boost_round = 150, \n",
    "                                    callbacks= [lg.early_stopping(stopping_rounds=20)])\n",
    "        \n",
    "        # make prediction     \n",
    "        self.test_predicted = self.regressor.predict(self.x_test)\n",
    "        self.train_predicted = self.regressor.predict(self.x_train)\n",
    "        \n",
    "        #save model\n",
    "        if os.path.exists(self.model_file) == False:\n",
    "            print(f'Saving model to file {self.model_file}. Data shape: {self.data.shape}. Train ROCAUC: {self.get_train_ROCAUC()}. Test ROCAUC: {self.get_test_ROCAUC()}')\n",
    "            pickle.dump(self.regressor, open(self.model_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f771c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeLGBMSelector(FeatureSelector):\n",
    "    \n",
    "    results_file = 'IterativeLGBMSelector_results.pcl'\n",
    "    model_file = 'IterativeLGBMSelector_model.pcl'\n",
    "    df_file = 'IterativeLGBMSelector_df.pcl'\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data:pd.DataFrame, \n",
    "                 selector:EnhancedLGBMSelector,\n",
    "                 base_line_auc:float,\n",
    "                 num_of_boosting_rounds:int,\n",
    "                 do_log:bool):\n",
    "        \n",
    "        self.results_file = f'___{base_line_auc}_{num_of_boosting_rounds}_{self.results_file}'\n",
    "        self.model_file = f'___{base_line_auc}_{num_of_boosting_rounds}_{self.model_file}'\n",
    "        self.df_file =  f'___{base_line_auc}_{num_of_boosting_rounds}_{self.df_file}'\n",
    "        \n",
    "        self.data = data\n",
    "        self.trained_selector = selector\n",
    "        self.base_line_auc = base_line_auc\n",
    "        self.num_of_boosting_rounds = num_of_boosting_rounds\n",
    "        self.do_log = do_log\n",
    "        \n",
    "        # check pre-saved results\n",
    "        if os.path.exists(self.results_file) == True:\n",
    "            \n",
    "            with open(self.results_file, 'rb') as file:\n",
    "                self.result = pickle.load(file)\n",
    "            \n",
    "            with open(self.model_file, 'rb') as file:\n",
    "                self.regressor = pickle.load(file)\n",
    "                \n",
    "            with open(self.df_file, 'rb') as file:\n",
    "                self.df = pickle.load(file)\n",
    "            \n",
    "            # save data for metrics\n",
    "            test, train = split_test_train(self.df)\n",
    "            self.y_train = train['target'].round(0).astype(int)\n",
    "            self.y_test = test['target'].round(0).astype(int)\n",
    "            self.x_train = train.drop(['target'], axis=1)\n",
    "            self.x_test = test.drop(['target'], axis=1)\n",
    "            \n",
    "            # predict\n",
    "            self.train_predicted = self.regressor.predict(self.x_train)\n",
    "            self.test_predicted = self.regressor.predict(self.x_test)\n",
    "            \n",
    "        else:\n",
    "            self.result = self.train_model()\n",
    "    \n",
    "    def get_regressor(self):\n",
    "        return self.regressor\n",
    "    \n",
    "    def get_training_results(self):\n",
    "        return self.result\n",
    "    \n",
    "    def train_model(self):\n",
    "        \n",
    "        all_features = [f[0] for f in self.trained_selector.get_important_features_tuples()]\n",
    "        do_log = True\n",
    "        existing = ['target', 'ind']\n",
    "        roc_auc = 0\n",
    "        prev_roc_auc = 0\n",
    "        increase_rate = 0\n",
    "        result = []\n",
    "\n",
    "        # for every column\n",
    "        for c in list(all_features):\n",
    "            if c != 'target':\n",
    "\n",
    "                # add column\n",
    "                existing.append(c)\n",
    "\n",
    "                # new dataset\n",
    "                df = df_combine[existing]\n",
    "\n",
    "                # build model\n",
    "                test, train = split_test_train(df)\n",
    "                y_train = train['target'].round(0).astype(int)\n",
    "                y_test = test['target'].round(0).astype(int)\n",
    "                x_train = train.drop(['target'], axis=1)\n",
    "                x_test = test.drop(['target'], axis=1)\n",
    "\n",
    "                # split train on train and validation\n",
    "                X_train_final, X_test_validation, y_train_final, y_test_validation = train_test_split(x_train, \n",
    "                                                                                                      y_train, \n",
    "                                                                                                      stratify=y_train,\n",
    "                                                                                                      test_size=0.25)\n",
    "        \n",
    "                # create datasets\n",
    "                d_train = lg.Dataset(X_train_final, label=y_train_final)\n",
    "                d_test = lg.Dataset(X_test_validation, label=y_test_validation, reference=d_train)\n",
    "\n",
    "                params={}\n",
    "                params['boosting_type']='dart' \n",
    "                params['objective']='binary' \n",
    "                params['metric']='auc' \n",
    "                params['verbosity'] = 0\n",
    "\n",
    "                # train the model \n",
    "                regressor=lg.train(params = params, \n",
    "                              train_set = d_train,\n",
    "                              valid_sets = d_test, \n",
    "                              num_boost_round = self.num_of_boosting_rounds)\n",
    "\n",
    "                # calculate metric\n",
    "                predicted = regressor.predict(x_train)\n",
    "\n",
    "                try:\n",
    "                    roc_auc = Metrics.roc_auc(y_train, predicted)\n",
    "                except ValueError:\n",
    "                    roc_auc = -1\n",
    "\n",
    "                # save result\n",
    "                diff = roc_auc - prev_roc_auc\n",
    "                item = (len(existing), existing, roc_auc, diff)\n",
    "                result.append(item)\n",
    "                if self.do_log == True: print(f'======== {len(existing)} -> AUC: {roc_auc} -> DIFF: {diff}' )\n",
    "\n",
    "                if (roc_auc > self.base_line_auc):\n",
    "                    if self.do_log == True: print(f'======== {roc_auc} increased base line threshold with {len(existing) -2} features {existing}')\n",
    "                    break\n",
    "\n",
    "                # handle negative impact\n",
    "                if (diff < -0.01):\n",
    "                    if self.do_log == True: print(f'======== Feature {c} gives negative impact of {diff}. Removing it.')\n",
    "                    existing.remove(c)\n",
    "                else:\n",
    "                    prev_roc_auc = roc_auc\n",
    "        \n",
    "        # save result to file\n",
    "        pickle.dump(result, open(self.results_file, 'wb'))\n",
    "        pickle.dump(regressor, open(self.model_file, 'wb'))\n",
    "        pickle.dump(df, open(self.df_file, 'wb'))\n",
    "        self.regressor = regressor\n",
    "        \n",
    "        # make prediction\n",
    "        self.train_predicted = self.regressor.predict(x_train)\n",
    "        self.test_predicted = self.regressor.predict(x_test)\n",
    "        \n",
    "        # data for metrics calc\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test \n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_heatmap(self) -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    def plot_heatmap(self):\n",
    "        pass\n",
    "    \n",
    "    def get_important_features(self) -> pd.DataFrame:\n",
    "        return self.result[-1][1][2:]\n",
    "    \n",
    "    def get_important_features_tuples(self) -> List[tuple[str, float, int]]:\n",
    "        return self.result[-1][1][2:]\n",
    "    \n",
    "    def get_test_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_test, self.test_predicted)\n",
    "\n",
    "    def get_train_ROCAUC(self):\n",
    "        return Metrics.roc_auc(self.y_train, self.train_predicted)\n",
    "\n",
    "    def get_test_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_confusion_matrix(self):\n",
    "        return Metrics.confusion_matrix(self.y_train, self.train_predicted)\n",
    "    \n",
    "    def get_test_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_test, self.test_predicted)\n",
    "    \n",
    "    def get_train_classification_report(self):\n",
    "        return Metrics.classification_report(self.y_train, self.train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710153e5",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effae5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformPipe:\n",
    "    \n",
    "    def __init__(self, funcs:List[Callable], **kwargs):\n",
    "        self.funcs = funcs\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def transform(self, df:DataFrame) -> DataFrame:\n",
    "        for f in self.funcs:\n",
    "            df = f(df, **self.kwargs)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76692925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs -> col_names_to_impute, verbose, impute_method\n",
    "def impute_numeric_cols(df:DataFrame, **kwargs) -> DataFrame:\n",
    "    \n",
    "    impute_method = kwargs['impute_method']\n",
    "    \n",
    "    col_names = kwargs['col_names_to_impute']\n",
    "    \n",
    "    # all possible columns\n",
    "    initial_cols = list(df.columns)\n",
    "    \n",
    "    for col_name in col_names:\n",
    "\n",
    "        non_empty_numeric =  (set(get_number_cols(df)) - set(get_empty_cols(df))) | {col_name}        \n",
    "        cols_names_to_drop = list((set(df.columns) - non_empty_numeric))\n",
    "        \n",
    "        if (kwargs['verbose']==True): print('Deleting: {0}. Imputing: {1}'.format(cols_names_to_drop, col_name))\n",
    "        \n",
    "        # save temp\n",
    "        temp = df[cols_names_to_drop]\n",
    "\n",
    "        # clear dataset \n",
    "        df = df.drop(columns=cols_names_to_drop)\n",
    "        \n",
    "        known = df.loc[ df[col_name].notnull() ]        \n",
    "        unknown = df.loc[ df[col_name].isnull() ]\n",
    "        \n",
    "        # nothing to predict\n",
    "        if (len(unknown) == 0): \n",
    "            if (kwargs['verbose']==True): print('Nothing to predict - continue')\n",
    "            continue\n",
    "        \n",
    "        column_index = list(df.columns).index(col_name)\n",
    "\n",
    "        all_indices = [i for i in range(unknown.shape[1])]\n",
    "        diff = list(set(all_indices) - {column_index})\n",
    "\n",
    "        y = known.values[:, column_index]\n",
    "        X = known.values[:, diff]\n",
    "\n",
    "        # select regressor\n",
    "        if impute_method == 'randomforest':\n",
    "            regressor = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        if impute_method == 'knn':\n",
    "            regressor = KNeighborsRegressor()\n",
    "            \n",
    "        regressor.fit(X, y)\n",
    "        predicted = regressor.predict(unknown.values[:, diff])\n",
    "        \n",
    "        if (kwargs['verbose']==True): print('{0} was predicted. Len: {1}'.format(col_name, len(predicted)))\n",
    "        \n",
    "        # fill missings\n",
    "        df.loc[ (df[col_name].isnull()), col_name ] = predicted\n",
    "        \n",
    "        # restore dataset\n",
    "        df[cols_names_to_drop] = temp\n",
    "    \n",
    "    # reorder columns back\n",
    "    df = df.reindex(columns = initial_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91279004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_skew(df:DataFrame, **kwargs) -> DataFrame:\n",
    "    \n",
    "    excluded_features = kwargs['excluded_features']\n",
    "    \n",
    "    skew_df = pd.DataFrame(get_number_cols(df), columns=['Feature'])\n",
    "    skew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(df[feature]))\n",
    "    skew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\n",
    "    skew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\n",
    "\n",
    "    for column in skew_df.query(\"Skewed == True\")['Feature'].values:\n",
    "        if (column not in excluded_features):\n",
    "            df[column] = np.log1p(df[column])\n",
    "        else:\n",
    "            print(f'Skipping column: {column}')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b6fea",
   "metadata": {},
   "source": [
    "### Threshold selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd7846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdHelper:\n",
    "    \n",
    "    def apply_binary_threshold(threshold:float, y_predicted):\n",
    "        result = []\n",
    "\n",
    "        for y in y_predicted:\n",
    "            if (y >= threshold):\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b2c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdSelector(abc.ABC):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_threshold(self, y_true, y_predicted) -> float:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cfb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ThresholdSelector(ThresholdSelector):\n",
    "    \n",
    "    def get_f1(self, y_real, y_predicted) -> float:\n",
    "        precision, recall, thresholds = precision_recall_curve(y_real, y_predicted)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        ix = argmax(fscore)\n",
    "        f_score = fscore[ix]\n",
    "        return f_score\n",
    "    \n",
    "    def get_threshold(self, y_real, y_predicted) -> float:\n",
    "        precision, recall, thresholds = precision_recall_curve(y_real, y_predicted)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        ix = argmax(fscore)\n",
    "        best_threshold = thresholds[ix]\n",
    "        return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd68b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceBasedThresholdSelector(ThresholdSelector):\n",
    "    \n",
    "    def __init__(self, add_new_abon_price:float, retain_abon_price:float, plot:bool):\n",
    "        self.plot = plot\n",
    "        self.add_new_abon_price = add_new_abon_price\n",
    "        self.retain_abon_price = retain_abon_price\n",
    "        self.plot \n",
    "    \n",
    "    def price_for_threshold(self, threshold) -> float:\n",
    "        ix = self.X.index(threshold)\n",
    "        return self.Y[ix]\n",
    "    \n",
    "    def get_threshold(self, y_real, y_predicted) -> float:\n",
    "        \n",
    "        X = []\n",
    "        TNs = []\n",
    "        FNs = []\n",
    "        TPs = []\n",
    "        FPs = []\n",
    "        Y = []\n",
    "        \n",
    "        for thr in np.arange (0, 1, 0.01):\n",
    "            test_predicted_final = ThresholdHelper.apply_binary_threshold(thr, y_predicted)\n",
    "            conf_m = Metrics.confusion_matrix(y_real, test_predicted_final)\n",
    "\n",
    "            TN = conf_m[0][0] \n",
    "            FN = conf_m[0][1]\n",
    "            TP = conf_m[1][1]\n",
    "            FP = conf_m[1][0]\n",
    "\n",
    "            final_price = TP*self.retain_abon_price + FP*self.add_new_abon_price + FN*self.add_new_abon_price\n",
    "\n",
    "            X.append(thr)            \n",
    "            TNs.append(TN)\n",
    "            FNs.append(FN)\n",
    "            TPs.append(TP)\n",
    "            FPs.append(FP)\n",
    "            Y.append(final_price)\n",
    "            \n",
    "        best_threshold = X[Y.index(min(Y))]        \n",
    "        f1_selector = F1ThresholdSelector()\n",
    "        f1_threshold = f1_selector.get_threshold(y_real, y_predicted)\n",
    "        \n",
    "        if self.plot == True:\n",
    "                    \n",
    "            threshold_price_X = [best_threshold, best_threshold]\n",
    "            threshold_price_Y = [0, max([*TPs, *FPs, *FNs])]\n",
    "            \n",
    "            threshold_f1_X = [f1_threshold, f1_threshold]\n",
    "            threshold_f1_Y = [0, max([*TPs, *FPs, *FNs])]\n",
    "            \n",
    "            # price\n",
    "            Metrics.XY_plot([X, threshold_f1_X, threshold_price_X],\n",
    "                            [Y, [0, max(Y)], [0, max(Y)]], \n",
    "                            ['Price', 'F1 threshold', 'Price threshold'],\n",
    "                            'threshold', 'price')\n",
    "\n",
    "            # metrics\n",
    "            Metrics.XY_plot([X, X, X, threshold_f1_X, threshold_price_X ],\n",
    "                            [TPs, FPs, FNs, threshold_f1_Y, threshold_price_Y ],\n",
    "                            ['TPs', 'FPs', 'FNs', 'F1 threshold', 'Price threshold'], \n",
    "                            'threshold', 'metrics')\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.best_threshold = best_threshold\n",
    "        \n",
    "        return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c2cb6",
   "metadata": {},
   "source": [
    "### Full experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "716c4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, \n",
    "                data:DataFrame,\n",
    "                selector:FeatureSelector, \n",
    "                transformer:TransformPipe):\n",
    "        self.data = data\n",
    "        self.selector = selector\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def run(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688eb39d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87d3baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_num, train_dpi, test_df, test_num, test_dpi = load_churn_reduced()\n",
    "df_combine, df_combine_num, df_combine_dpi = combine_data(train_df, train_num, train_dpi, test_df, test_num, test_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f98699",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5393bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43\n",
    "# https://www.kaggle.com/code/gomes555/tps-jun2021-feature-selection-lightgbm-tuner\n",
    "\n",
    "pearsonSelector = CorrelationSelector(df_combine, 'pearson', -1)\n",
    "spearmanSelector = CorrelationSelector(df_combine, 'spearman', -1)\n",
    "normal = LGBMSelector(df_combine, -1)\n",
    "enhanced = EnhancedLGBMSelector(df_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4a0d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.954024923900115, 0.8980686013446386)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.get_train_ROCAUC(), normal.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b7a538b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9310064543985634, 0.8964773842119418)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced.get_train_ROCAUC(), enhanced.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e748f917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 27)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_non_corr = pearsonSelector.get_important_noncorrelated_features_tuples(0.95, 0.2)\n",
    "spearman_non_corr = spearmanSelector.get_important_noncorrelated_features_tuples(0.95, 0.2)\n",
    "len(pearson_non_corr), len(spearman_non_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfada5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8916336158130531, 0.8543018544980374)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [*[f[0] for f in pearson_non_corr], *['target', 'ind']]\n",
    "df_temp = df_combine[features]\n",
    "pearson_test_selector = EnhancedLGBMSelector(df_temp, '___pearson_enhanced.plc')\n",
    "pearson_test_selector.get_train_ROCAUC(), pearson_test_selector.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c607f80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.878686640995647, 0.8449606682497676)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [*[f[0] for f in spearman_non_corr], *['target', 'ind']]\n",
    "df_temp = df_combine[features]\n",
    "spearman_test_selector = EnhancedLGBMSelector(df_temp, '___spearman_enhanced.plc')\n",
    "spearman_test_selector.get_train_ROCAUC(), spearman_test_selector.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0ed60d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iterative_150 \u001b[38;5;241m=\u001b[39m \u001b[43mIterativeLGBMSelector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.89\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m iterativeFeatures \u001b[38;5;241m=\u001b[39m iterative_150\u001b[38;5;241m.\u001b[39mget_important_features()\n\u001b[0;32m      3\u001b[0m results_150 \u001b[38;5;241m=\u001b[39m iterative_150\u001b[38;5;241m.\u001b[39mget_training_results()\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mIterativeLGBMSelector.__init__\u001b[1;34m(self, data, selector, base_line_auc, num_of_boosting_rounds, do_log)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   3536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3537\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 3538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3539\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3540\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    846\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 848\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:908\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[1;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    907\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 908\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_API_IS_ROW_MAJOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterative_150 = IterativeLGBMSelector(df_combine, normal, 0.89, 150, True)\n",
    "iterativeFeatures = iterative_150.get_important_features()\n",
    "results_150 = iterative_150.get_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_150.get_train_ROCAUC(), iterative_150.get_test_ROCAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [r[0] for r in results_150]\n",
    "Y = [r[2] for r in results_150]\n",
    "Metrics.XY_plot([X], [Y], ['Num of features VS AUC'], 'Num of features', 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2baa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [f'Default {df_combine.shape[1]}',\n",
    "     f'Cross-val {df_combine.shape[1]}',\n",
    "     f'Pearson non-cor {len(pearson_non_corr)}',\n",
    "     f'Spearman non-cor {len(spearman_non_corr)}',\n",
    "     f'Iterative {len(iterativeFeatures)}' ]\n",
    "\n",
    "Y = [normal.get_test_ROCAUC(),\n",
    "     enhanced.get_test_ROCAUC(),\n",
    "     pearson_test_selector.get_test_ROCAUC(),\n",
    "     spearman_test_selector.get_test_ROCAUC(),\n",
    "     iterative_150.get_test_ROCAUC() ]\n",
    "\n",
    "Metrics.BAR_plot(X, Y, 'Models AUC comparison', 'Model name', 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e692ee",
   "metadata": {},
   "source": [
    "## Filling Missing Data\n",
    "### Selecting iterative model, because it showed best AUC with less num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures = [*[f for f in iterativeFeatures], *['target', 'ind']]\n",
    "df_final = df_combine[finalFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_path = '___final_dataset_knn.pcl'\n",
    "\n",
    "if os.path.exists(imputed_path) == True:\n",
    "    df_final = pickle.load(open(imputed_path, 'rb'))\n",
    "else:\n",
    "    # kwargs -> col_names_to_impute, verbose, impute_method\n",
    "    df_final = impute_numeric_cols(df_final, col_names_to_impute = get_empty_cols(df_final), verbose = True, impute_method = 'knn')\n",
    "    pickle.dump(df_final, open(imputed_path, 'wb'))\n",
    "    \n",
    "df_final = remove_skew(df_final, excluded_features = ['target'])\n",
    "kkn_no_skew = EnhancedLGBMSelector(df_final, 'kkn_no_skew.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_path = '___final_dataset_randomforest.pcl'\n",
    "\n",
    "if os.path.exists(imputed_path) == True:\n",
    "    df_final = pickle.load(open(imputed_path, 'rb'))\n",
    "else:\n",
    "    # kwargs -> col_names_to_impute, verbose, impute_method\n",
    "    df_final = impute_numeric_cols(df_final, col_names_to_impute = get_empty_cols(df_final), verbose = True, impute_method = 'randomforest')\n",
    "    pickle.dump(df_final, open(imputed_path, 'wb'))\n",
    "    \n",
    "df_final = remove_skew(df_final, excluded_features = ['target'])\n",
    "randomforest_no_skew = EnhancedLGBMSelector(df_final, 'randomforest_no_skew.pcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ 'Iterative' + ' ' + str(len(iterativeFeatures)),\n",
    "      'Knn No Skew',\n",
    "      'Random Forest No Skew' ]\n",
    "\n",
    "Y = [ results[-1][2], kkn_no_skew.get_ROCAUC(), randomforest_no_skew.get_ROCAUC() ]\n",
    "  \n",
    "Metrics.BAR_plot(X, Y, 'Models AUC comparison', 'Model name', 'AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59513712",
   "metadata": {},
   "source": [
    "### Threshold + Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/96690/how-to-choose-the-right-threshold-for-binary-classification\n",
    "# https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/\n",
    "# https://ploomber.io/blog/threshold/\n",
    "\n",
    "Metrics.plot_auc([normal.y_train, normal.y_test], [normal.train_predicted,  normal.test_predicted], ['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65930111",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_selector = F1ThresholdSelector()\n",
    "f1_threshold = f1_selector.get_threshold(normal.y_train, normal.train_predicted)\n",
    "print(f'F1 based threshold: {f1_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_based_selector = PriceBasedThresholdSelector(1000, 50, True)\n",
    "price_based_thr = price_based_selector.get_threshold(normal.y_train, normal.train_predicted)\n",
    "print(f'Price based threshold: {price_based_thr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_price = price_based_selector.price_for_threshold(round(f1_threshold, 2))\n",
    "price_based = price_based_selector.price_for_threshold(round(price_based_thr, 2))\n",
    "print(f'f1 price: {f1_price}. Price based: {price_based}. Diff: {f1_price - price_based}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86210570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_f1_threshold = ThresholdHelper.apply_binary_threshold(f1_threshold, normal.train_predicted)\n",
    "ConfusionMatrixDisplay.from_predictions(normal.y_train, with_f1_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fe7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_price_threshold = ThresholdHelper.apply_binary_threshold(price_based_thr, normal.train_predicted)\n",
    "ConfusionMatrixDisplay.from_predictions(normal.y_train, with_price_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab493d",
   "metadata": {},
   "source": [
    "### Check if dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned = len(df_combine[df_combine['target'] == 1])\n",
    "not_churned = len(df_combine[df_combine['target'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [churned, not_churned]\n",
    "x = ['Churned', 'Not churned']\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd1142",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "#### Check inbound calls from non-vodaphone number in min\n",
    "#### Check outbound calls to non-vodaphone number in min\n",
    "#### SMS from non-vodaphone number count\n",
    "#### SMS to non-vodaphone number count\n",
    "#### Calls to pawnshop, micro credit organizations in mins\n",
    "#### Calls from pawnshop, micro credit organizations in mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8d68b",
   "metadata": {},
   "source": [
    "## Explore numbers abonent had communication with + frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f46ce76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned = df_combine[df_combine['target'] == 1]\n",
    "churned_with_nums = pd.merge(churned, df_combine_num, on='abon_id', how='left')\n",
    "churned_numbers = list(churned_with_nums['bnum'].unique())\n",
    "\n",
    "non_churned = df_combine[df_combine['target'] == 0]\n",
    "non_churned_with_nums = pd.merge(non_churned, df_combine_num, on='abon_id', how='left')\n",
    "non_churned_numbers = list(non_churned_with_nums['bnum'].unique())\n",
    "\n",
    "number_abon_had_communicated = (set(churned_numbers) - set(non_churned_numbers))\n",
    "df_number_abon_had_communicated = pd.DataFrame(number_abon_had_communicated, columns= ['bnum'])\n",
    "\n",
    "temp = churned_with_nums[churned_with_nums['bnum'].isin(list(number_abon_had_communicated))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "775756a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers churned abonents communicated with: [b'test', b'admiralinfo', b'foodera', b'autogranit', b'707', b'3103', b'maracao', b'biotestlab', b'2552', b'torross', b'7300', b'tehnotop', b'meganomer', b'5135', b'7610', b'erudito', b'tepmihobo', b'9444', b'sumuiu', b'clinica mc', b'amkr-info', b'if-toys.com', b'zakupkiprom', b'usavitamin', b'rast-brak', b'hexcarbon', b'wirwetten', b'5170', b'dobroludya', b'lombard zf', b'fczenit', b'money_point', b'7001', b'uaswitch', b'moto', b'taxi elit', b'5515', b'3113', b'gett', b'sanagro', b'ektaservice', b'0578', b'onverify', b'internetma', b'concordia', b'moya kuhny', b'4908', b'diamarket', b'ledproector', b'avtoznak', b'mxsms', b'lino home', b'whitecredi', b'5454', b'mayak-medoc', b'shopintriga', b'domoscope', b'6218', b'correos', b'8313', b'7500', b'9405', b'430', b'affiliate', b'lombgotivka', b'1107', b'solar', b'tviybarber', b'ecotrade', b'brook clean', b'foxdental', b'dobroludyam', b'3600', b'awt dnepr', b'vsezaimy', b'inbox.eu', b'change', b'5281', b'pudra.stor', b'zakupkipro', b'trade admin', b'eka clinic', b'timeshop', b'vrapp', b'mobihel', b'support.vo', b'tvoyapozyk']\n"
     ]
    }
   ],
   "source": [
    "short_nums = [t for t in number_abon_had_communicated if TelephoneHelper.is_short_number(t) or TelephoneHelper.is_string_number(t)]\n",
    "print(f'Numbers churned abonents communicated with: {short_nums}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61332b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__PAWNSHOPS__ = ['lombard zf', 'money_point', 'whitecredi', 'vsezaimy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8aeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34549bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4836c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fab878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe1c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cf0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde2220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88836c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c81d9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers churned abonents communicated with: {b'380679183818', b'380487051800', b'380445278653', b'test', b'380937502050', b'admiralinfo', b'380674060778', b'380504731498', b'380954062313', b'380675430405', b'380445013009', b'380973203040', b'380573702158', b'380674871339', b'380444562043', b'380676127363', b'380636333323', b'foodera', b'380505033629', b'autogranit', b'707', b'380563731910', b'380966895077', b'380445276910', b'380503870100', b'380954934466', b'380983303305', b'3103', b'380442286369', b'380978769984', b'380509339314', b'380973345411', b'380635714246', b'380686165555', b'maracao', b'380444558887', b'biotestlab', b'2552', b'torross', b'380503320440', b'380506436075', b'7300', b'380959075869', b'380959414164', b'tehnotop', b'380632300327', b'380975223397', b'380965697783', b'380633069848', b'380974941492', b'380577585058', b'380507638553', b'meganomer', b'380951000830', b'380506756015', b'5135', b'7610', b'erudito', b'380634291694', b'380567454017', b'380677442332', b'380960750909', b'tepmihobo', b'380673511116', b'9444', b'sumuiu', b'380955967373', b'380639714247', b'clinica mc', b'380661916366', b'380982255440', b'380982636506', b'380637977035', b'380964658465', b'amkr-info', b'380506533240', b'380965312608', b'380445155023', b'if-toys.com', b'zakupkiprom', b'380956768562', b'usavitamin', b'380675656262', b'380679698430', b'380660203050', b'rast-brak', b'380665381308', b'380683718651', b'380987783249', b'380675653391', b'380445663427', b'380444961496', b'380675596968', b'380955636375', b'380503009050', b'380976332233', b'380503889890', b'380964142622', b'380973237850', b'380663509397', b'380991019853', b'380675650967', b'380503423348', b'380445278742', b'380933471753', b'380445949780', b'380504951515', b'hexcarbon', b'380676660466', b'wirwetten', b'380504237872', b'5170', b'dobroludya', b'380504030594', b'380503909383', b'380673560346', b'380637252335', b'lombard zf', b'380994729196', b'380509125365', b'380681817548', b'380502230311', b'380675686221', b'380638773131', b'380978510293', b'380632193318', b'380505946780', b'fczenit', b'380971409009', b'380500506296', b'380937748748', b'380445022010', b'money_point', b'380637967418', b'380984531051', b'380976449577', b'7001', b'380933147514', b'380674842217', b'uaswitch', b'380631303001', b'380963435666', b'moto', b'taxi elit', b'380958505858', b'380509508048', b'380676171105', b'380972184644', b'380979774316', b'380995292056', b'5515', b'380674882895', b'380938252220', b'3113', b'gett', b'380980253475', b'380935614603', b'380675607200', b'380673284188', b'380994865838', b'380961799999', b'380504634187', b'380459551126', b'380970040444', b'380442790962', b'sanagro', b'380950256766', b'ektaservice', b'380933939000', b'380935215321', b'380671177330', b'380632479399', b'380678927800', b'380988111483', b'380662081740', b'380995624742', b'380677080557', b'380630455466', b'380677679060', b'380674830060', b'380509181875', b'380675039771', b'380672497279', b'0578', b'onverify', b'380676177212', b'380688112674', b'internetma', b'380937673223', b'380672093018', b'380960332232', b'concordia', b'380964789907', b'moya kuhny', b'380679044753', b'380504431615', b'380961219840', b'380932539096', b'380956708828', b'380661563333', b'380675760754', b'4908', b'380487374528', b'380503649060', b'380935230082', b'diamarket', b'ledproector', b'380959363203', b'avtoznak', b'mxsms', b'380442371358', b'380487058720', b'lino home', b'380992232125', b'whitecredi', b'5454', b'380675572231', b'380442370372', b'380504634252', b'20035', b'380671558066', b'380976359212', b'380986551165', b'380674457255', b'380984214193', b'380982091494', b'380939011842', b'380979060539', b'mayak-medoc', b'shopintriga', b'380675562221', b'380676112680', b'380979888000', b'380965741710', b'domoscope', b'380676191743', b'380930595155', b'380504194944', b'380678806317', b'380504816868', b'6218', b'380956577714', b'380675189605', b'correos', b'380674066689', b'8313', b'380931392459', b'380930319716', b'380504922009', b'380506461775', b'380672954847', b'380958659131', b'380979610928', b'7500', b'380487525698', b'380675089046', b'380444565002', b'9405', b'380674617808', b'380677742579', b'430', b'380677605018', b'380442270911', b'380667038370', b'affiliate', b'380676315158', b'380506309353', b'lombgotivka', b'1107', b'380678773993', b'380638646725', b'380988410769', b'380989444341', b'380567569586', b'380630102300', b'380983259811', b'380972913250', b'380954199432', b'380675731234', b'380979386603', b'380685626660', b'380678263331', b'solar', b'tviybarber', b'380977052367', b'380972648223', b'ecotrade', b'brook clean', b'380505332709', b'380674833392', b'foxdental', b'380975771276', b'dobroludyam', b'380681760275', b'380674432521', b'380661765702', b'380509067108', b'380687840706', b'380487946239', b'380459561534', b'3600', b'awt dnepr', b'380672921382', b'380667486149', b'380445317877', b'380665757570', b'vsezaimy', b'380972265070', b'380660379919', b'380667147317', b'380950340533', b'inbox.eu', b'change', b'380991204905', b'10200', b'380994817557', b'380504004648', b'380487294741', b'380632602854', b'380444635745', b'5281', b'pudra.stor', b'zakupkipro', b'380936346455', b'380990158664', b'380509355928', b'trade admin', b'380677488603', b'380966587660', b'380977805357', b'380502915474', b'eka clinic', b'380676864461', b'380930976013', b'380444865271', b'380931755504', b'380683103434', b'380445668384', b'380961370609', b'timeshop', b'vrapp', b'380677919805', b'380675779111', b'380668054255', b'380679520201', b'mobihel', b'380937569973', b'380680960154', b'380632955772', b'380577252482', b'support.vo', b'380933819681', b'tvoyapozyk'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f993d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_non_vodafone_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (df_combine_num['call_dur_in'] > 0) &\n",
    "                          (df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone))]\n",
    "    \n",
    "    return sum(list(data['call_dur_in']))\n",
    "\n",
    "def get_outgoing_non_vodafone_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (df_combine_num['call_dur_out'] > 0) &\n",
    "                          (df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone))]\n",
    "    \n",
    "    return sum(list(data['call_dur_in']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce7ae365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_non_sms_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (df_combine_num['cnt_sms_in'] > 0) &\n",
    "                          (df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone))]\n",
    "    \n",
    "    return sum(list(data['cnt_sms_in']))\n",
    "\n",
    "def get_outgoing_non_sms_nums(abon_id):\n",
    "    \n",
    "    data = df_combine_num[(df_combine_num['abon_id'] == abon_id) & \n",
    "                          (df_combine_num['cnt_sms_out'] > 0) &\n",
    "                          (df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone))]\n",
    "    \n",
    "    return sum(list(data['cnt_sms_out']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81c6b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_combine[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc207ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['incoming_call_non_vodafone'] = df_tmp['abon_id'].apply(get_incoming_non_vodafone_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "faba1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['outgoing_call_non_vodafone'] = df_tmp['abon_id'].apply(get_outgoing_non_vodafone_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['incoming_sms_non_vodafone'] = df_tmp['abon_id'].apply(get_incoming_non_sms_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['outgoing_sms_non_vodafone'] = df_tmp['abon_id'].apply(get_outgoing_non_sms_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87716bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5704dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1160870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dccb397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ec7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_num[(df_combine_num['abon_id'] == 1625156) & (df_combine_num['call_dur_in'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb09d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4540bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_num.query('call_dur_in > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_num['bnum'].apply(TelephoneHelper.is_non_vodafone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebdd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf87ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebfc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a31fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2663e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee5937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbd090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cda91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9c129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f561238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849679a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb011672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b0253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844d34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57e4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4583527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b13eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0afbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['AAAAAAAAA'] = df_tmp['abon_id'].apply(get_incoming_non_vodafone_numns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dca7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp[df_tmp['AAAAAAAAA'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe01480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2571df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8b706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95266521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd1634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805932f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d32be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601567ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715dcff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fc7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8595539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c5ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf720d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9096c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f421c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaf039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b41454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb9be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1a936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491732e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_combine_num.query('abon_id == {1585461} and call_cnt_out > 0')['bnum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaa65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0ce16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5b6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53135ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fa7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9291315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188adc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5345d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e215f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8736d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14476208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168844f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682faf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbe4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdcb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7432d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76590191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c7fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fd5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inbound calls\n",
    "df_combineх['abon_id'].apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549173c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80765d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6290bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239570a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bf93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d756312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176ae72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f72c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75354c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255c810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49518bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61e2d077",
   "metadata": {},
   "source": [
    "## Telephone Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75958788",
   "metadata": {},
   "source": [
    "## Groupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = train_num.groupby(['abon_id'])\n",
    "gr.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6992251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_matrix = train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c437c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca83cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6700b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facebe50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad075db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c644d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9765e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d167d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e5976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbbe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84dcf0a",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
